\subsection{Proposed approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The main purpose of this work is to investigate the feasibility of deep learning approaches for delamination identification in CFRP materials by only utilizing frames of the full wavefield propagation of the guided waves.
Accordingly, two deep learning models based on time data sequence were developed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=.2\textheight]{Fully_ConvLSTM2d_MODEL_updated.png}
		\caption{Convolutional LSTM model}
		\label{fig:convlstm_model}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=.2\textheight]{RNN_LSTM_MODEL_updated.png}
		\caption{Time distributed AE model}
		\label{fig:AE_convlstm}
	\end{subfigure}
	\caption{The architecture of the proposed deep learning models.}
	\label{fig:proposed_models}
\end{figure} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The developed models presented in Fig.~\ref{fig:proposed_models} have a scheme of Many-to-One sequence prediction, in which it takes \(n\) number of frames representing the waves propagation through time and their interaction with the damage in order to extract the damage features, and finally predicts the delamination location, shape and size in a single output image.

The first proposed model presented in~\ref{fig:convlstm_model} 
consists of a three ConvLSTM layers with the filter sizes of 10, 5, and 10 respectively.
The kernel size of the ConvLSTM layers was set to (\(3\times3\)) with a stride of 1 padding was set to "same" which makes the output as same as the input in the case of stride 1.
Furthermore, a \(tanh\) (the hyperbolic tangent) activation function was used at the ConvLSTM layers, which outputs a values in a range between \(-1\) and \(1\).
Moreover, batch normalization~\cite{Santurkar2018} was also applied after each ConvLSTM layer.  
The final layer is used a simple 2D convolutional layer followed by a sigmoid activation function which outputs values in a range between \(0\) and \(1\) to indicate the delamination probability.
In this model, we applied a binary cross-entropy as the objective function.
Moreover,Adadelta~\cite{zeiler2012adadelta} optimization technique was utilised that performs back-propagation through time (BPTT)~\cite{goodfellow2016deep}. 
Since the sigmoid activation function produces probability values between \(0\) and \(1\), a threshold value must be chosen to classify the output into a damaged or undamaged classes.
Accordingly, the threshold value was set to (\(0.5\)).
The reason for choosing this value for the sigmoid activation function is explained in our previous research work~\cite{ijjeh2021full}.

In the second model presented in~\ref{fig:AE_convlstm} we have applied an autoencoder technique (AE) which is well-known technique for features extraction.
The idea of AE is to compress the input data within the encoding process then learn how to reconstruct it back from the reduced encoded representation (latent space) to a representation that is as close to the original input as possible. 
Accordingly, AE reduces data dimensions by learning how to discard the noise in the data.
In this model, we have investigated the use of AE to process a sequence of input frames in order to perform image segmentation operation.
Accordingly, a Time Distributed layer presented in Fig.~\ref{fig:TD} was introduced to the model, in which it distributes the input frames into the AE to keep the independently among frames.
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\textwidth]{Time_ditributed_layer.png}
	\caption{Flow of input frames using Time distributed layer}
	\label{fig:TD}
\end{figure}

The AE consists of three parts: the encoder, the bottleneck, and the decoder.
The encoder is responsible for learning how to reduce the input dimensions and compress the input data into an encoded representation.
In Fig.~\ref{fig:AE_convlstm}, the encoder part consists of four levels of downsampling. 
The purpose of having different scale levels is to extract feature maps from the input image at different scales.
Every level at the encoder consists of two 2D convolution operations followed by a Batch Normalization then a dropout is applied. 
Furthermore, at the end of each level a Maxpooling operation is applied to reduce the dimensionality of the inputs. 
The bottleneck presented in Fig.\ref{fig:AE_convlstm} has the lowest level of dimensions of the input data, further it consists of two 2D convolution operations followed by a Batch Normalization.
The decoder part presented in Fig.\ref{fig:AE_convlstm}, is responsible of learning how to restore the original dimensions of the input.
The decoder part consists of two 2D convolutional operations followed by Batch Normalization and Dropout, and an upsampling operation is applied at the end of each decoder level to retrieve the dimensions of its inputs.
Moreover, to enhance the performance learning of the decoder skip connections linking the encoder with the corresponding decoder levels were added.
The outputs sequences of the decoder part is fed into the ConvLSTM2D layer that is utilized to learn long-term spatiotemporal features.
Finally, a 2D convolution operations is applied on the output of the ConvLSTM2d layer followed by a sigmoid activation function.