%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{FCN-DenseNet model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
The one hundred layer tiramisu model (FCN-DenseNet) was introduced by Simon Jegou et al.~\cite{Jegou} for semantic segmentation.
FCN-DenseNet is similar to the U-Net architecture, FCN-DenseNet utilises the U-shape of the encoder-decoder scheme with skip connections between the downsampling and the upsampling paths to increase the resolution to the final feature map.
%Skip connections from the downsampling path to the corresponding upsampling path are essential for recovering spatially detailed information by reusing feature maps.

The main component in FCN-DenseNet is the dense block.
The purpose of the dense block is to concatenate layer input (feature maps) with its output (feature maps) to emphasize spatial details information.
The dense block is constructed from \(n\) varying number of layers, each layer is composed of a series of operations.
Figure~\ref{dense_block} illustrates the architecture of the dense block.
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0,angle=-90]{DenseBlock_layer.png}
	\end{center}
	\caption{Dense block architecture.} 
	\label{dense_block}
\end{figure}
%It has an input (\(x\)) (input image or output of transition layer) with \(k\) feature maps which is concatenated with the output of first layer and this process is recursively performed for all layers in the dense block ending up with output (\(y\)) with a (\(n\times k\)) feature maps. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Transition down layer was introduced to reduce the spatial dimensionality of the feature maps by performing a (\(1\times 1\)) convolution followed by (\(2\times2\)) Maxpool operation. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For the transition up layer, it was introduced in FCN-DenseNet to recover the input spatial resolution, to do that a transpose convolution operation is performed which upsamples the previous feature maps.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Feature maps emerging from upsampling are concatenated with the ones resulting from the skip connection forming the input to a new dense block.
During the upsampling, the input to the dense block is not concatenated with its output to overcome the overhead of memory shortage since the upsampling path expands the spatial resolution of the feature maps.
Table~\ref{layers} presents the architecture of a single layer, the transition down  and transition up layers in details.
Figure~\ref{fcn} illustrates the FCN-DenseNet architecture for image segmentation used for delamination detection.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.3}
	\centering
	\scriptsize
	\begin{tabular}{|c|l|c|l|c|}
		\cline{1-1} \cline{3-3} \cline{5-5}
		\textbf{Layer} &  & \textbf{Transition Down} &  & \textbf{Transition Up} \\ \cline{1-1} \cline{3-3} \cline{5-5} 
		Batch Normalization &  & Batch Normalization &  & \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}(\(3\times3\)) Transposed Convolution, \\ strides = (\(2\times2\))\end{tabular}} \\ \cline{1-1} \cline{3-3}
		Relu &  & Relu &  &  \\ \cline{1-1} \cline{3-3}
		(\(3\times3\)) Convolution &  & (\(1\times1\)) Convolution &  &  \\ \cline{1-1} \cline{3-3}
		\multirow{2}{*}{Dropout \(p = 0.2\)} &  & Dropout \(p = 0.2\) &  &  \\ \cline{3-3}
		&  & (\(2\times2\)) Maxpooling &  &  \\ \cline{1-1} \cline{3-3} \cline{5-5} 
	\end{tabular}
	\caption{Layer, Transition Down and Transition Up layers.} 
	\label{layers}
\end{table}\\

\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale= 1.0]{FCN_dense_net.png}
	\end{center}
	\caption{FCN-DenseNet architecture.} 
	\label{fcn}
\end{figure}
%Our constructed model is composed of \(3\) dense blocks in the downsampling path, one dense block in bottleneck and 3 dense blocks for the upsampling path. 
%Each dense block in the downsampling and upsampling paths consists of \(2\) layers, the bottleneck dense block consists of \(4\) layers.
%The model input is the RMS image with size of (\(512\times 512\)).
%At the beginning, we perform a  convolution operation and concatenate the original input with the output, then the concatenated output is fed into the first dense block that consists of (\(2\)) layers.
%
%Each layer is composed of Ronneberger2015normalization (BN) followed by Relu, then (\(3\times3\)) convolution with same padding is applied followed by a dropout with probability \(p = 0.2\).
%Then, the output of the first dense layer is concatenated with its input and is fed into a transition down layer. 
%
%The transition down layer is composed of BN followed by Relu, then (\(1\times1\)) convolution followed by a dropout with probability \(p = 0.2\) and finally (\(2\times2\)) Maxpool with strides of (\(2\times2\)).
%
%This process is repeated until the bottleneck dense block.
%The bottleneck dense block consists of 4 layers.
%The output of the bottleneck is directed into the upsampling path starting with transition up layer.
%Accordingly, the output of the transition up layer is concatenated with the corresponding dense block output in the downsampling path.
%The final layer in the network is  (\(1\times1\)) convolution followed by either a sigmoid function or a softmax function to calculate the probability of damage for each pixel.
%Hence, we have two versions of the FCN-DenseNet model with different output layer function.
