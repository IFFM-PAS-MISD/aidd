\subsection{Deep learning models}
\label{proposed_approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this work, we developed two end-to-end deep learning models utilising full 
wavefield frames of Lamb wave propagation for delamination identification in 
CFRP materials, as presented in Figure~\ref{fig:proposed_models}.
The developed models have a scheme of many-to-one sequence prediction, which takes \(m\) number of frames representing the full wavefield propagation through time and their interaction with the delamination to extract the damage features and finally predict the delamination location, shape, and size in a single output image.
The proposed deep learning models were implemented on Keras API~\cite{chollet2015keras} running on top of TensorFlow on a Tesla V100 GPU from NVIDIA.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=.2\textheight]{figure5a.png}
		\caption{\centering Model-\RNum{1}} % : Convolutional LSTM model.
		\label{fig:convlstm_model}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=.2\textheight]{figure5b.png}
		\caption{\centering Model-\RNum{2}} % : Time distributed AE model.
		\label{fig:AE_convlstm}
	\end{subfigure}
	\caption{The architecture of the proposed deep learning models.}
	\label{fig:proposed_models}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The first proposed model, presented in Figure~\ref{fig:convlstm_model} takes 
\(64\) frames as input, and it consists of three ConvLSTM layers that can 
process time series and computer vision tasks.
The first ConvLSTM layer has \(12\) filters, the second layer has \(6\) filters, and the third layer has \(12\) filters.
The kernel size of the ConvLSTM layers was set to (\(3\times3\)) with a stride of \((1)\). 
Padding was set to "same", which makes the output the same as the input in the case of stride \(1\).
Furthermore, a \(\tanh\) (the hyperbolic tangent) activation function was used within the ConvLSTM layers that output values in the range between (\(-1\) and \(1\)).
Moreover, we applied a batch normalization technique~\cite{Santurkar2018} after the first two ConvLSTM layers.

In the second implemented model presented in Figure~\ref{fig:AE_convlstm}, we 
applied an autoencoder technique (AE) which is well-known for extracting 
spatial features.
The idea of AE is to compress the input data within the encoding process and then learn how to reconstruct it back from the reduced encoded representation (latent space) to a representation that is as close to the original input as possible. 
In this model, we have investigated the use of AE to process a sequence of \(24\) frames to perform image segmentation.
Therefore, a time-dispersed layer, presented in Figure~\ref{fig:TD} was 
introduced to the model, in which it distributes the input frames into the AE 
layers in order to process them independently.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.45\textwidth]{figure6.png}
	\caption{Flow of input frames using Time-distributed layer.}
	\label{fig:TD}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In general, an AE consists of three parts: the encoder, the bottleneck, and the decoder.
The encoder is responsible for learning how to reduce the input dimensions and compress the input data into an encoded representation.
In Figure~\ref{fig:AE_convlstm}, the encoder part consists of four levels of 
downsampling. 
The purpose of having different scale levels is to extract feature maps from the input image at different scales.
Every level at the encoder consists of two 2D convolution operations followed by a Batch Normalization, and then a Dropout is applied.
Furthermore, at the end of each level, a Maxpooling operation is applied to reduce the dimensionality of the inputs. 
The bottleneck presented in Figure~\ref{fig:AE_convlstm} has the lowest level 
of dimensions of the input data.
It consists of two 2D convolution operations followed by batch normalization.
The decoder part presented in Figure~\ref{fig:AE_convlstm} is responsible for 
learning how to restore the original dimensions of the input.
The decoder part consists of two 2D convolutional operations followed by batch normalization and dropout, and an upsampling is applied at the end of each decoder level to retrieve the dimensions of its inputs.
Skip connections linking the encoder with the corresponding decoder levels were added to enhance the feature extraction process.
The outputs of the decoder were forwarded into the ConvLSTM2D layer to learn long-term spatio-temporal features.

In both models, we applied a 2D convolutional layer as the final output layer followed by a sigmoid activation function that outputs values in a range from \((0,1)\) to indicate the delamination probability.
Consequently, a threshold value must be chosen to classify the output into damaged (represented by \(1\)) or undamaged (represented by \(0\)).
Hence, we set the threshold value to (\(0.5\)) to exclude all values below the threshold by considering them as undamaged and taking only those values greater than the threshold to be considered as damaged.

For evaluating the performance of the proposed models, the mean 
intersection over union \(IoU\) (Jaccard index) was applied as the accuracy metric. 
\(IoU\) is estimated by determining the intersection
area between the ground truth and the predicted output. 
Further, we have two output classes (damaged and undamaged), the \(IoU\) was calculated for the damaged class only. 
Equation~(\ref{eqn:iou}) defines the \(IoU\) metric: 
\begin{equation}
	IoU=\frac{Intersection}{Union}=\frac{\hat{Y} \cap Y}{\hat{Y} \cup Y},
	\label{eqn:iou}
\end{equation}
where \(\hat{Y}\) is the predicted output, and \(Y\) is the ground truth.
Additionally, the percentage area error $\epsilon$ depicted in 
equation~(\ref{eqn:mean_size_error}) was utilised to evaluate the performance 
of the models:
\begin{equation}
	\epsilon=\frac{|A-\hat{A}|}{A} \times 100\%,
	\label{eqn:mean_size_error}
\end{equation}
where \(A\) and \(\hat{A}\) refer to the area in mm\textsuperscript{2} of the damage class in the ground truth and the predicted output, respectively.
This metric can indicate how close the area of the predicted delamination is to the ground truth.
Accordingly, the lower the value of $\epsilon$, the higher the accuracy of the identified damage. 
Furthermore, for all predicted outputs, the delamination localisation error (the distance between the delamination centres of the GT and the predicted output) was less than \(0.001\%\), hence, it is not considered in the discussion section.