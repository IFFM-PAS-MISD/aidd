\subsection{Deep learning models}
\label{proposed_approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this work, we developed two end-to-end deep learning models utilising full wavefield frames of Lamb wave propagation for delamination identification in CFRP materials\DIFaddbegin \DIFadd{, }\DIFaddend as presented in Fig.~\ref{fig:proposed_models}.
The developed models have a scheme of many-to-one sequence prediction, which takes \DIFdelbegin \DIFdel{\(n\) }\DIFdelend \DIFaddbegin \DIFadd{\(m\) }\DIFaddend number of frames representing the full wavefield propagation through time and their interaction with the delamination to extract the damage features \DIFdelbegin \DIFdel{, }\DIFdelend and finally predict the delamination location, shape, and size in a single output image.
\DIFaddbegin \DIFadd{The proposed deep learning models were implemented on Keras API~\mbox{%DIFAUXCMD
\cite{chollet2015keras} }\hspace{0pt}%DIFAUXCMD
running on top of TensorFlow on a Tesla V100 GPU from NVIDIA.
}\DIFaddend %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=.2\textheight]{figure3a.png}
%DIFDELCMD < 		%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=.2\textheight]{figure5a.png}
		\DIFaddendFL \caption{Model-\RNum{1}} % : Convolutional LSTM model.
		\label{fig:convlstm_model}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=.2\textheight]{figure3b.png}
%DIFDELCMD < 		%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=.2\textheight]{figure5b.png}
		\DIFaddendFL \caption{Model-\RNum{2}} % : Time distributed AE model.
		\label{fig:AE_convlstm}
	\end{subfigure}
	\caption{The architecture of the proposed deep learning models.}
	\label{fig:proposed_models}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The first proposed model\DIFaddbegin \DIFadd{, }\DIFaddend presented in Fig.~\ref{fig:convlstm_model} \DIFaddbegin \DIFadd{takes \(64\) frames as input, and it }\DIFaddend consists of three ConvLSTM layers that can process time series and computer vision tasks.
The first ConvLSTM layer has \(12\) filters, \DIFaddbegin \DIFadd{the }\DIFaddend second layer has \(6\) filters\DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{, and the }\DIFaddend third layer has \(12\) filters.
The kernel size of the ConvLSTM layers was set to (\(3\times3\)) with a stride of \DIFdelbegin \DIFdel{\(1\), padding }\DIFdelend \DIFaddbegin \DIFadd{\((1)\). 
Padding }\DIFaddend was set to "same"\DIFaddbegin \DIFadd{, }\DIFaddend which makes the output the same as the input in the case of stride \(1\).
Furthermore, a \(\tanh\) (the hyperbolic tangent) activation function was used within the ConvLSTM layers that output values in \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend range between (\(-1\) and \(1\)).
Moreover, we applied a \DIFdelbegin \DIFdel{Batch Normalization }\DIFdelend \DIFaddbegin \DIFadd{batch normalization }\DIFaddend technique~\cite{Santurkar2018} after the first two ConvLSTM layers.

In the second \DIFaddbegin \DIFadd{implemented }\DIFaddend model presented in Fig.~\ref{fig:AE_convlstm}\DIFaddbegin \DIFadd{, }\DIFaddend we applied an autoencoder technique (AE) which is well-known for extracting spatial features.
The idea of AE is to compress the input data within the encoding process \DIFaddbegin \DIFadd{and }\DIFaddend then learn how to reconstruct it back from the reduced encoded representation (latent space) to a representation that is as close to the original input as possible. 
In this model, we have investigated the use of AE to process a sequence of \DIFdelbegin \DIFdel{input }\DIFdelend \DIFaddbegin \DIFadd{\(24\) }\DIFaddend frames to perform image segmentation.
Therefore, a \DIFdelbegin \DIFdel{Time Distributed layer}\DIFdelend \DIFaddbegin \DIFadd{time-dispersed layer, }\DIFaddend presented in Fig.~\ref{fig:TD} was introduced to the model, in which it distributes the input frames into the AE layers in order to process them independently.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=0.5\textwidth]{figure4.png}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=0.5\textwidth]{figure6.png}
	\DIFaddendFL \caption{Flow of input frames using \DIFdelbeginFL \DIFdelFL{Time distributed }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Time-distributed }\DIFaddendFL layer.}
	\label{fig:TD}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In general, an AE consists of three parts: the encoder, the bottleneck, and the decoder.
The encoder is responsible for learning how to reduce the input dimensions and compress the input data into an encoded representation.
In Fig.~\ref{fig:AE_convlstm}, the encoder part consists of four levels of downsampling. 
The purpose of having different scale levels is to extract feature maps from the input image at different scales.
Every level at the encoder consists of two 2D convolution operations followed by a Batch Normalization\DIFaddbegin \DIFadd{, and }\DIFaddend then a Dropout is applied.
Furthermore, at the end of each level\DIFaddbegin \DIFadd{, }\DIFaddend a Maxpooling operation is applied to reduce the dimensionality of the inputs. 
The bottleneck presented in Fig.~\ref{fig:AE_convlstm} has the lowest level of dimensions of the input data\DIFdelbegin \DIFdel{, further it }\DIFdelend \DIFaddbegin \DIFadd{.
It }\DIFaddend consists of two 2D convolution operations followed by \DIFdelbegin \DIFdel{a Batch Normalization}\DIFdelend \DIFaddbegin \DIFadd{batch normalisation}\DIFaddend .
The decoder part presented in Fig.~\ref{fig:AE_convlstm} \DIFdelbegin \DIFdel{, }\DIFdelend is responsible for learning how to restore the original dimensions of the input.
The decoder part consists of two 2D convolutional operations followed by \DIFdelbegin \DIFdel{Batch Normalization and Dropout}\DIFdelend \DIFaddbegin \DIFadd{batch normalization and dropout}\DIFaddend , and an upsampling \DIFdelbegin \DIFdel{operation }\DIFdelend is applied at the end of each decoder level to retrieve the dimensions of its inputs.
Skip connections linking the encoder with the corresponding decoder levels were added to enhance the \DIFdelbegin \DIFdel{features }\DIFdelend \DIFaddbegin \DIFadd{feature }\DIFaddend extraction process.
The outputs of the decoder were forwarded into the ConvLSTM2D layer to learn long-term \DIFdelbegin \DIFdel{spatiotemporal }\DIFdelend \DIFaddbegin \DIFadd{spatio-temporal }\DIFaddend features.

In both models, we applied a 2D convolutional layer as the final output layer followed by a sigmoid activation function \DIFdelbegin \DIFdel{which }\DIFdelend \DIFaddbegin \DIFadd{that }\DIFaddend outputs values in a range from \((0,1)\) to indicate the delamination probability.
Consequently, a threshold value must be chosen to classify the output into \DIFdelbegin \DIFdel{a damaged represented by (}\DIFdelend \DIFaddbegin \DIFadd{damaged (represented by }\DIFaddend \(1\)) or undamaged \DIFdelbegin \DIFdel{represented by (}\DIFdelend \DIFaddbegin \DIFadd{(represented by }\DIFaddend \(0\)).
Hence, we set the threshold value to (\(0.5\)) to exclude all values below the threshold by considering them as undamaged and taking only those values greater than the threshold to be considered as damaged.
%DIF < The reason for selecting such threshold value to the sigmoid activation function is explained in our previous research work~\cite{Ijjeh2021}.
%DIF < Further, Adadelta~\cite{zeiler2012adadelta} optimization method with binary cross-entropy (BCE) was applied to the first model, whereas in the second model, Adam optimizer~\cite{Kingma2014} with BCE was applied.

For evaluating the performance of the proposed models, the mean 
intersection over union \(IoU\) (Jaccard index) was applied as the accuracy metric. 
\(IoU\) is estimated by determining the intersection
area between the ground truth and the predicted output. 
Further, we have two output classes (damaged and undamaged), the \(IoU\) was calculated for the damaged class only. 
Equation~(\ref{eqn:iou}) \DIFdelbegin \DIFdel{illustrated }\DIFdelend \DIFaddbegin \DIFadd{defines }\DIFaddend the \(IoU\) metric: 
\begin{equation}
	IoU=\frac{Intersection}{Union}=\frac{\hat{Y} \cap Y}{\hat{Y} \cup Y}\DIFaddbegin \DIFadd{,
	}\DIFaddend \label{eqn:iou}
\end{equation}
where \(\hat{Y}\) is the predicted output, and \(Y\) is the ground truth.
%DIF < %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%DIF < \begin{table}[]
%DIF < 	\centering
%DIF < 	\caption{Models parameters}
%DIF < 	\label{tab:parmeters}
%DIF < 	\begin{tabular}{ccc}
%DIF < 		\hline
%DIF < 		\multirow{2}{*}{Parameter} & \multicolumn{2}{c}{Model}               
%DIF < 		\\ 
%DIF < 		\cline{2-3} 
%DIF < 		& 1  & 2
%DIF < 		\\ \hline
%DIF < 		Optimizer &	Adadelta~\cite{zeiler2012adadelta} &  Adam~\cite{Kingma2014}              
%DIF < 		\\
%DIF < 		Batch size & 2  & 1                
%DIF < 		\\
%DIF < 		Dropout  & 0.0  & 0.2                
%DIF < 		\\
%DIF < 		Learning rate & 1.0 & 0.01             
%DIF < 		\\
%DIF < 		Loss function  & \multicolumn{2}{c}{Binary cross entropy} 
%DIF < 		\\ \hline
%DIF < 	\end{tabular}
%DIF < \end{table}
\DIFaddbegin \DIFadd{Additionally, the percentage area error $\epsilon$ depicted in Eq.~(\ref{eqn:mean_size_error}) was utilised to evaluate the performance of the models:
}\begin{equation}
	\DIFadd{\epsilon=\frac{|A-\hat{A}|}{A} \times 100\%,
	\label{eqn:mean_size_error}
}\end{equation}
\DIFadd{where \(A\) and \(\hat{A}\) refer to the area in mm\textsuperscript{2} of the damage class in the ground truth and the predicted output, respectively.
This metric can indicate how close the area of the predicted delamination is to the ground truth.
Accordingly, the lower the value of $\epsilon$, the higher the accuracy of the identified damage. 
Furthermore, for all predicted outputs, the delamination localisation error (the distance between the delamination centres of the GT and the predicted output) was less than \(0.001\%\), hence, it is not considered in the discussion section. }\DIFaddend