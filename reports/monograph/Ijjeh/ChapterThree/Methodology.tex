\section{Methodology}
\label{methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset}
The synthetically generated dataset~\cite{Ijjeh2021} was utilised for training several DL models.
The dataset resembles velocity measurements obtained by SLDV in the transverse direction (perpendicular to the plate surface).
The dataset contains 475 simulated full wavefield scans of propagating Lamb waves in an eight-layer CFRP plate with a total thickness of \((3.9)\) mm interacting with delamination.
%%%%%%%%%%%%%%%

Delamination is represented by two shell layers that are only connected at the damaged area's boundary.
The corresponding, reduced, number of laminae and their offset from the plate's neutral axis are used to calculate these elements.
This method is described in detail in~\cite{Kudela2009}, has the advantage of keeping the computing efficiency of the model as compared to solid elements, as the increase in nodes is usually small.
In addition, the mass matrix's favorable attribute of being diagonal is maintained. 
%%%%%%%%%%%%%%%
Each simulated scenario depicts the interaction of Lamb waves with a single delamination, which was modeled using random parameters as like spatial location, size, and orientation.
Figure~\ref{fig:rand_delaminations} shows a plate with 475 delamination cases overlayed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure1.png}
	\end{center}
	\caption{The plate with 475 cases of random delaminations.} 
	\label{fig:rand_delaminations}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The amplitudes of the propagating waves at location \((x,y)\) and time \((t)\) are stored in each simulated case in the dataset generated from the wave propagation model.
As a result, these matrices can be viewed as animated frames of propagating waves at discrete time \((t_k)\).
Furthermore, it should be noted that the simulated delaminations were closer to the top of the plate surface.
As a result, utilizing the simulated full wavefield on the top surface of the plate instead of the bottom surface makes it easier to detect delamination.
However, we used the more difficult situation of registering the entire wavefield at the bottom surface of plate to train several models.
The Root Mean Square (RMS) depicted in Eq.~\ref{ref:rms} was applied to the full wavefield frames in order to improve the visualisation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	\hat{s}(x,y) = \sqrt{\frac{1}{N}\sum_{k=1}^{N} s(x,y,t_k)^2}
	\label{ref:rms}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where \(N\) denotes the number of sampling points (N=512).
The results of applying RMS to the full wavefield from the top and bottom surfaces of the plate can be seen in~\ref{fig:rmstop} and \ref{fig:rmsbottom}, respectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}		
		\centering
		\includegraphics[scale=.29]{figure2a.png}
		\caption{top}
		\label{fig:rmstop}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}		
		\centering
		\includegraphics[scale=.29]{figure2b.png}
		\caption{bottom}
		\label{fig:rmsbottom}
	\end{subfigure}
	\caption{RMS of the full wavefield from the top surface of the plate (a) and the bottom surface of the plate (b).}
	\label{fig:rms}
\end{figure} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data preprocessing}
To improve the performance of the optimizer throughout the training phase, the color scale values were standardized to a range of \((0-1)\) instead of the initial scale which was in the range of \((0 - 255)\).
Additionally, the dataset was augmented by flipping the photos horizontally, vertically, and diagonally.
As a result, the dataset expanded four times in size, yielding (1900) image frames.
Further, the dataset was divided into two parts: the training set $(80\%)$ and the testing set $(20\%)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Moreover, a K-folds cross-validation method was applied to the training set to reduce the overfitting which happens when the model is able to fit on the training data, while it poorly fit on the new unseen data.
In other words, the model only learns the patterns of the training data therefore the model will not generalise well. 
Figure.~\ref{fig:Cross_validation} illustrates the K-fold CV technique.
In this technique, we have split the training set into \(K\) small sets (folds), hence the name K-folds. 
Therefore, we iterate over the training set K iterations.
During each iteration, the model uses  \(K-1\) folds for training and the remaining fold is used for validation. 
In our models, we have chosen \(K=5\), therefore, we have \(5\) iterations of training. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For each iteration, we compute the performance of the model.
Finally, the iteration with highest performance is selected.
The main advantage of the K-folds method versus a regular train/test split is to reduce the overfitting by utilising data more efficiently as every data sample is used in both training and validation. 
Therefore, by using this technique, we aim to improve the ability of the model to generalise and reduce the overfitting.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
	\centering
	\includegraphics[scale=1.0]{cross_validation.png}
	\caption{K-fold Cross validation, K=\(5\).}
	\label{fig:Cross_validation}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Semantic segmentation models}
\label{section:semantic_segmentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
DL approaches have advanced quickly in recent years in a variety of real-world applications, such as computer vision.
The technique of image segmentation is well-known in the field of computer vision.
This technique aims to assign a class to each pixel in the input image, and it is used in a variety of real-world applications like self-driving automobiles, medical imaging, traffic management systems, video surveillance, and more.
In this chapter, five DL models based on Fully Convolutional Networks (FCN)~\cite{Shelhamer2017a} we compared in order to identify and localize delamination in composite plates.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Similarly, these models attempt to perform pixel-wise segmentation by classifying each pixel in the input image as damaged or undamaged.
FCN can be generated by combining convolutional layers and skipping dense layers in an encoder-decoder architecture.
The encoder is aims to produce compressed feature maps from the input image at various scale levels using cascaded convolutions and downsampling operations.
While the decoder is responsible for upsampling the condensed feature maps to the original input shape.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this work, the softmax activation function was applied at the output layer for all implemented models in this comparative study.
The softmax calculates the probability for each predicted output of being damaged or undamaged for every single pixel, which implies that the sum of the two probabilities must be one. 
Eq.~(\ref{softmax}) depicts the softmax activation function, where \(P(x)_{i}\) is the probability of each target class \(x_{j}\) over all possible target classes \(x_{j}\), C in our case are two classes  (damaged and undamaged).
To predict the label of the output (\(y_{pred}\)) an argmax function is applied to select the maximum probability between both of them.
\begin{equation}
	P(x)_{i} = \frac{e^{x_{i}}}{\sum_{j}^{C} e^{x_{j}}}
	\label{softmax}
\end{equation} 
\begin{equation}
	y_{pred} = argmax_{i}\left( P(x)_{i} \right)
	\label{argmax}
\end{equation}
Selecting a suitable loss function is an important issue because it measures how well the model learns and performs.
Therefore, in all models, we have applied the categorical cross-entropy (CCE) loss function~\cite{Bonaccorso2020}, which is also called \enquote{softmax loss function}.
CCE is used as the objective function to estimate the difference between the actual damage (ground truth) and the predicted damage.
Further, since we have only two classes to be predicted, it is worth mentioning that a Sigmoid activation function at the output layer can be used with a binary cross-entropy (BCE), with no impacts on the predicted outputs.
Eq.~(\ref{CCE}) illustrates the CCE, where \( P(x)_{i}\) is the softmax value of the target class. 
\begin{equation}	
	CCE = -\log\left( P(x)_{i} \right)
	\label{CCE}
\end{equation}

Additionally, it is also important to select a proper accuracy metric of the model, therefore, we have applied intersection over union (\(IoU\)) (Jaccard index)~\cite{Bertels2019} as our accuracy metric. 
\(IoU\) is estimated by determining the intersection area between the ground truth and the predicted output.
In this work, we have two classes (damaged and undamaged), the \(IoU\) is computed by taking the \(IoU\) for the damaged class only.
The \(IoU\) metric is defined as in Eq.~(\ref{IoU}):
\begin{equation}
	IoU = \frac{Intersection}{Union} = \frac{\hat{Y} \cap Y}{\hat{Y} \cup Y} 
	\label{IoU}
\end{equation}
where \(\hat{Y}\) represents the predicted vector of damaged and undamaged values, and \(Y\) represents the vector of ground truth values.
The \(IoU\) can be calculated by multiplying the predicted output (matrix of \(zeros\) and \(ones\)) with its ground truth (matrix of \(zeros\) and \(ones\)) to find the intersection, then it is divided over the union which can be calculated by counting all pixels with non-zero values of the predicted output and its ground truth.

Furthermore, Adam optimizer was applied as our optimization method in order to increase the \(IoU\) and to reduce the loss during the training.
Figure~\ref{fig:flowchart} presents a diagram of the implemented DL models for pixel-wise semantic segmentation for delaminations identification. 
The details of the implemented models will be explained in detail in the next sections.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure3.png}
	\end{center}
	\caption{Schematic diagram of the approach used for comparison of semantic segmentation methods accuracy.} 
	\label{fig:flowchart}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Residual UNet model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Residual UNet (Res-UNet) model is based on the residual learning~\cite{He2016} and the UNet technique~\cite{Ronneberger2015}. 
It is a well-known architecture that performs a biomedical segmentation. 
The Res-UNet has a U-shape convolutional network that is based on encoder-decoder style. 
The architecture of Res-UNet is presented in Fig.~\ref{fig:Unet}.
The encoder (contracting) path is responsible for capturing the detailed context of an input image, while the decoder (expansive) path is responsible for enabling precise localisation. 
Hence, to maintain the spatial and contextual information from the previous layers from being lost residual connections were added at two levels:
\begin{itemize}
	\item at each step of the encoder and decoder paths,
	\item between the encoder parts and their corresponding decoder parts (skip connections) which ensures that the feature maps which were learned during the downsampling will be utilized in the reconstruction. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The encoder part holds several downsampling (Max-pool) blocks. 
Each block applies two convolutional layers followed by a (\(2\times2\)) max pooling with a (\(2\times2\)) strides that picks the maximum value in a local pool filter in one feature map (or \(n\)-feature maps), resulting in a reduction in the dimension of feature maps~\cite{Lecun2015}, consequently, reducing computation complexity.
Each convolutional layer performs (\(3\times3\)) convolution operations, followed by batch normalization (BN) then a Relu is applied.
Moreover, the number of convolutional filters is doubled after each downsampling block therefore the model can learn complex patterns effectively. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The bottleneck layer lies in between the encoder and the decoder as a joining point in the deepest layer in the model.
The bottleneck contains two convolutional layers, with \(1024\) filters which helps the model to learn and recognize the complex patterns.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The decoder consists of several upsampling blocks. 
Each upsampling block passes the input into two convolution layers as in the downsampling block followed by a transmission up layer consisting of a transposed convolutional layer (upsampling). 
The purpose of upsampling is to retrieve the dimensions and increase the resolution.
Transposed convolutional layer differs from the regular upsampling function, by introducing learnable parameters regarding the transposed convolution filters that enhance the learning process of the model. 
Moreover, after each upsampling operation, the number of feature maps used by the convolutional layer is reduced by half to keep the model symmetrical. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure4.png}
	\end{center}
	\caption{Res-UNet architecture.} 
	\label{fig:Unet}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{VGG16 encoder-decoder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this model, we address the use of VGG16~\cite{Simonyan2015} architecture as a backbone encoder to the UNet~\cite{Ronneberger2015} technique.
VGG16 is composed of 13 convolutional layers, pooling layers and \(3\) dense layers, and it is used for classification purposes.
We removed the dense layers from the model, and we applied VGG16 of 13 convolutional layers as encoder-decoder for pixel-wise image segmentation.
Figure~\ref{vgg16} presents the architecture of VGG16 encoder-decoder model. 
The model has a U-shape of two parts: encoder and decoder.
The encoder consists of \(5\) convolutional blocks with a total \(13\)  (\(3\times3\)) convolutional layers followed by BN and activation function Relu.
A Max pool operation with a pool size of (\(2\times2\)) followed by dropout is performed after each convolutional block.  
The upsampling path is introduced to recover spatial resolution, it also has \(5\) convolutional blocks with a total \(13\) \((3\times 3)\) convolutional layers.
For upsampling, bilinear interpolation with (\(2\times2\)) kernel size is applied.
Skip connections were added between downsampling blocks and the corresponding upsampling blocks in order to enhance recovering fine-grained details by enabling feature re-usability from earlier layers.
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure5.png}
	\end{center}
	\caption{VGG16 encoder decoder architecture.} 
	\label{vgg16}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{FCN-DenseNet model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
FCN-DenseNet which was introduced in~\cite{Jegou} was applied in our previous work~\cite{Ijjeh2021} for image segmentation.
The results were promising, since it outperformed the conventional damage detection technique i.e (adaptive wavenumber filtering method). 
FCN-DenseNet has a U-shape of the encoder-decoder scheme with skip connections between the downsampling and the upsampling paths to increase the resolution to the final feature map.
The main component in FCN-DenseNet is the dense block.
The dense block is constructed from \(n\) varying number of layers, each layer consists of a series of operations as shown in Table~\ref{layers}.
The purpose of the dense block is to concatenate the input (\(x\)) (feature maps) of a layer  with its output (feature maps) to emphasize spatial details information.
In this work, we have updated the FCN-DenseNet model by increasing the number of dense blocks and the learnable parameters (filters).
The architecture of the dense block is presented in Fig.~\ref{dense_block}. 
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0,angle=-90]{figure6.png}
	\end{center}
	\caption{Dense block architecture.} 
	\label{dense_block}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To reduce the spatial dimensionality of the produced feature maps, a transition down layer was added to perform a (\(1\times 1\)) convolution followed by (\(2\times2\)) Maxpooling operation. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consequently, to recover the spatial resolution, a transition-up layer was added. 
It applies a transpose convolution operation to upsample feature maps from the previous layer.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Feature maps emerging from upsampling are concatenated with the ones resulting from the skip connection forming the input to a new dense block.
During the upsampling, the input to the dense block is not concatenated with its output to overcome the overhead of memory shortage since the upsampling path expands the spatial resolution of the feature maps. 
%(hint:- We can refer to previous paper and say that the same architecture was applied here. Than we can skip Fig.~\ref{fcn}).
%\begin{figure} [h!]
%	\begin{center}
%		\includegraphics[scale=1.0]{FCN_dense_net.png}
%	\end{center}
%	\caption{FCN-DenseNet architecture.} 
%	\label{fcn}
%\end{figure}
Table~\ref{layers} presents the architecture of a single layer, the transition down  and transition up layers in details.
%Figure~\ref{fcn} illustrates the FCN-DenseNet architecture for image segmentation used for delamination detection.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.3}
	\centering
	\scriptsize
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{ccccc}
			\hline
			Layer &  &  Transition Down &  &  Transition Up \\ 
			\hline
			Batch Normalization &  & Batch Normalization &  &  \(3\times 3\) Transposed Convolution  \\ 
			Relu &  & Relu &  & strides = (\(2\times2\))  \\ 
			(\(3\times3\)) Convolution &  & (\(1\times1\)) Convolution &  &  \\ 
			%		&  &   \\ 
			Dropout \(p=0.2\) &  &Dropout \(p=0.2\)  &  &  \\ 
			&  & (\(2\times2\)) Maxpooling &  &  \\ 
			\hline
		\end{tabular}
	}
	\caption{Layer, Transition Down and Transition Up layers.} 
	\label{layers}
	
\end{table}\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Pyramid Scene Parsing Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Pyramid Scene Parsing Network (PSPNet) was proposed by Zhao et al.~\cite{zhao2017pyramid} .
It is one of the most advanced semantic segmentation techniques. 
The principle idea of PSPNet is to provide adequate global contextual information for pixel-level scene parsing through concatenating the local and global features together. 
Hence, a spatial pyramid pooling module was introduced to perform four different pooling levels with four different pooling sizes and strides.
In this way, the pyramid pooling module is able to capture contextual features from different scales.

In this work, we implemented PSPNet with ResNet-50~\cite{He2016} as a backbone for feature map extraction with dilation at the last two layers of ResNet. 
Figure~\ref{fig:PSPNet} illustrates the implemented PSPNet architecture.
The pyramid pooling module was applied on four levels.
Global average pooling was used to produce the coarsest level of a single bin output shown in the red box. 
The other three sub-region levels have different pooling sizes of \((2\times 2), (4\times 4)\) and \((8\times8)\).
A \(1 \times 1\) convolutional layer was applied to the produced feature maps to reduce their dimensions, followed by a BN and Relu.
Then, the feature maps produced from the different levels were upsampled with bilinear interpolation.
Moreover, the upsampled features are concatenated with the output of the ResNet-50 model to obtain both local and global context information. 
Next, 2 convolutional layers were used for generating the pixel-wise segmented predictions. 
\begin{figure} [h!]
	\centering
	\includegraphics[scale=1.0]{figure7.png}
	\caption{PSPNet architecture.} 
	\label{fig:PSPNet}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Global Convolutional Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Global Convolutional Network (GCN) proposed by Peng et al. 
~\cite{Peng2017} addressed the importance to have large kernels for both 
localisation and classification for semantic segmentation to enlarge 
respective fields.
However, a contradiction arises when performing classification and localisation 
tasks. 
For instance, classification tasks require the models to be invariant for 
different transformations such as rotation and translation.
On the other hand, localisation tasks require the models to be sensitive for 
any transformation, to accurately assign each pixel for its semantic category.
Accordingly, to solve this contradiction, two design principles were suggested: 
\begin{enumerate}
	\item For the classification task, in order to improve the capability of 
	the model to handle different transformations, a large kernel size must be 
	used to enable dense connections between feature maps and per-pixel 
	classifiers.
	\item For localisation task, the model must be fully convolutional. 
	Additionally, fully connected or global pooling layers are not applied as 
	these layers will discard the localisation information. 
\end{enumerate}

Figure~\ref{fig:gcn} presents the proposed GCN module for semantic segmentation 
utilised for delamination identification.
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure8.png}
	\end{center}
	\caption{Global Convolution Network whole architecture.} 
	\label{fig:gcn}
\end{figure}
As shown in the Fig.~\ref{fig:gcn}, a residual network was utilised as a backbone for 
feature maps extraction, the residual block is shown in 
Fig.~\ref{fig:res_gcn_br}a.
After each residual block, a GCN block is inserted  
(Fig.~\ref{fig:res_gcn_br}b), which employs a combination of \((1\times 
k)\)+\((k\times 1)\) and \((k\times 1)\)+\((1\times k)\) convolutions which 
enables a dense connections within a large \((k\times k)\) region in the 
feature map.
In this work, we implemented the model with \(k=7\).
This is followed by a boundary refinement (BR) block shown in Fig.~\ref{fig:res_gcn_br}c, which can be considered as an additional residual block to refine the predictions near the object boundaries ended up generating a lower resolution score map. 
Furthermore, the upsampling operation is done recursively, it upsamples the low 
resolution score maps then concatenate it with a higher one to produce a new 
score maps.
The deconvolution operation is repeated until the original image size is 
obtained.
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure9.png}
	\end{center}
	\caption{(a) Residual block, (b) Global Convolution Network block, (c) 
		Boundary Refinement} 
	\label{fig:res_gcn_br}
\end{figure}