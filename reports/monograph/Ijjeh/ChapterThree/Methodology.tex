\section{Methodology}
\label{methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset}
The synthetically generated dataset~\cite{Ijjeh2021} was utilised for training several DL models.
The dataset resembles velocity measurements obtained by SLDV in the transverse direction (perpendicular to the plate surface).
The dataset contains 475 simulated full wavefield scans of propagating Lamb waves in an eight-layer CFRP plate with a total thickness of \((3.9)\) mm interacting with delamination.
%%%%%%%%%%%%%%%

Delamination is represented by two shell layers that are only connected at the damaged area's boundary.
The corresponding, reduced, number of laminae and their offset from the plate's neutral axis are used to calculate these elements.
This method is described in detail in~\cite{Kudela2009}, has the advantage of keeping the computing efficiency of the model as compared to solid elements, as the increase in nodes is usually small.
In addition, the mass matrix's favorable attribute of being diagonal is maintained. 
%%%%%%%%%%%%%%%
Each simulated scenario depicts the interaction of Lamb waves with a single delamination, which was modeled using random parameters as like spatial location, size, and orientation.
Figure~\ref{fig:rand_delaminations} shows a plate with 475 delamination cases overlayed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure1.png}
	\end{center}
	\caption{The plate with 475 cases of random delaminations.} 
	\label{fig:rand_delaminations}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The amplitudes of the propagating waves at location \((x,y)\) and time \((t)\) are stored in each simulated case in the dataset generated from the wave propagation model.
As a result, these matrices can be viewed as animated frames of propagating waves at discrete time \((t_k)\).
Furthermore, it should be noted that the simulated delaminations were closer to the top of the plate surface.
As a result, utilizing the simulated full wavefield on the top surface of the plate instead of the bottom surface makes it easier to detect delamination.
However, we used the more difficult situation of registering the entire wavefield at the bottom surface of plate to train several models.
The Root Mean Square (RMS) depicted in Eq.~\ref{ref:rms} was applied to the full wavefield frames in order to improve the visualisation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	\hat{s}(x,y) = \sqrt{\frac{1}{N}\sum_{k=1}^{N} s(x,y,t_k)^2}
	\label{ref:rms}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where \(N\) denotes the number of sampling points (N=512).
The results of applying RMS to the full wavefield from the top and bottom surfaces of the plate can be seen in~\ref{fig:rmstop} and \ref{fig:rmsbottom}, respectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}		
		\centering
		\includegraphics[scale=.29]{figure2a.png}
		\caption{top}
		\label{fig:rmstop}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}		
		\centering
		\includegraphics[scale=.29]{figure2b.png}
		\caption{bottom}
		\label{fig:rmsbottom}
	\end{subfigure}
	\caption{RMS of the full wavefield from the top surface of the plate (a) and the bottom surface of the plate (b).}
	\label{fig:rms}
\end{figure} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data preprocessing}
To improve the performance of the optimizer throughout the training phase, the color scale values were standardized to a range of \((0-1)\) instead of the initial scale which was in the range of \((0 - 255)\).
Additionally, the dataset was augmented by flipping the photos horizontally, vertically, and diagonally.
As a result, the dataset expanded four times in size, yielding (1900) image frames.
Further, the dataset was divided into two parts: the training set $(80\%)$ and the testing set $(20\%)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Moreover, a K-folds cross-validation method was applied to the training set to reduce the overfitting which happens when the model is able to fit on the training data, while it poorly fit on the new unseen data.
In other words, the model only learns the patterns of the training data therefore the model will not generalise well. 
Figure.~\ref{fig:Cross_validation} illustrates the K-fold CV technique.
In this technique, we have split the training set into \(K\) small sets (folds), hence the name K-folds. 
Therefore, we iterate over the training set K iterations.
During each iteration, the model uses  \(K-1\) folds for training and the remaining fold is used for validation. 
In our models, we have chosen \(K=5\), therefore, we have \(5\) iterations of training. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For each iteration, we compute the performance of the model.
Finally, the iteration with highest performance is selected.
The main advantage of the K-folds method versus a regular train/test split is to reduce the overfitting by utilising data more efficiently as every data sample is used in both training and validation. 
Therefore, by using this technique, we aim to improve the ability of the model to generalise and reduce the overfitting.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
	\centering
	\includegraphics[scale=1.0]{cross_validation.png}
	\caption{K-fold Cross validation, K=\(5\).}
	\label{fig:Cross_validation}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Semantic segmentation models}
\label{section:semantic_segmentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
DL approaches have advanced quickly in recent years in a variety of real-world applications, such as computer vision.
The technique of image segmentation is well-known in the field of computer vision.
This technique aims to assign a class to each pixel in the input image, and it is used in a variety of real-world applications like self-driving automobiles, medical imaging, traffic management systems, video surveillance, and more.
In this chapter, five DL models based on Fully Convolutional Networks (FCN)~\cite{Shelhamer2017a} we compared in order to identify and localize delamination in composite plates.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Similarly, these models attempt to perform pixel-wise segmentation by classifying each pixel in the input image as damaged or undamaged.
FCN can be generated by combining convolutional layers and skipping dense layers in an encoder-decoder architecture.
The encoder is aims to produce compressed feature maps from the input image at various scale levels using cascaded convolutions and downsampling operations.
While the decoder is responsible for upsampling the condensed feature maps to the original input shape.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The softmax activation function was used at the output layer for all constructed models.
For every single pixel, the softmax estimates the probability of each predicted output being damaged or undamaged, implying that the sum of the two probabilities must be one.
The softmax activation function is depicted by Eq.~(\ref{softmax}), where \(P(x)_{i}\) is the probability of each target class \(x_{j}\) across all potential target classes \(x_{j}\), C in our instance being two classes (damaged and undamaged).
An argmax function is used to find the maximum probability between each of them in order to predict the label of the output (\(y_{pred}\)).
\begin{equation}
	P(x)_{i} = \frac{e^{x_{i}}}{\sum_{j}^{C} e^{x_{j}}}
	\label{softmax}
\end{equation} 
\begin{equation}
	y_{pred} = argmax_{i}\left( P(x)_{i} \right)
	\label{argmax}
\end{equation}
Choosing the right loss function is crucial since it determines how effectively the model learns and performs.
As a result, the categorical cross-entropy (CCE) loss function~\cite{Bonaccorso2020}, commonly known as the \enquote{softmax loss function}, was utilised.
The difference between the actual damage (ground truth) and the expected damage is estimated using CCE as the objective function.
Furthermore, because there are only two classes to predict, a Sigmoid activation function at the output layer can be combined with a binary cross-entropy (BCE) without affecting the anticipated outputs.
The CCE is illustrated by Eq.~(\ref{CCE}), where  \( P(x)_{i}\) refers to the softmax value of the target class.
\begin{equation}	
	CCE = -\log\left( P(x)_{i} \right)
	\label{CCE}
\end{equation}

Further, it is vital to choose an appropriate accuracy metric for the training purposes.
Hence, intersection over union \((IoU)\) (Jaccard index)~\cite{Bertels2019} was utilised to measure the accuracy of prediction during training process.
The intersection area between the ground truth and the predicted output is used to estimate \((IoU)\).
In this work, the (IoU) is calculated solely for the damaged class label.
Eq.~(\ref{IoU}) defines the (IoU) metric as follows:
\begin{equation}
	IoU = \frac{Intersection}{Union} = \frac{\hat{Y} \cap Y}{\hat{Y} \cup Y} 
	\label{IoU}
\end{equation}
where \(\hat{Y}\) refers to the predicted tensor values, while \(Y\) refers to the tensor of ground truth values.

Moreover, the Adam optimizer was used as our optimizer to improve the (IoU) and decrease the loss during training.
The implemented DL models for pixel-wise semantic segmentation for delaminations identification are depicted in Figure~\ref{fig:flowchart}.
In the following, five DL models will be illustrated.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure3.png}
	\end{center}
	\caption{Schematic diagram of the approach used for comparison of semantic segmentation methods accuracy.} 
	\label{fig:flowchart}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Residual UNet model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Residual UNet (Res-UNet) model was inspired based on residual learning~\cite{He2016} and UNet approaches~\cite{Ronneberger2015}.
The Res-UNet architecture is depicted in Fig.~\ref{fig:Unet}.
The encoder (compressive) path aims to capture the detailed features of an input image, whereas the decoder (decompressive) path aims to perform exact localization.
As a result, residual connections were established at two levels in order to prevent the spatial and contextual information from the preceding layers from being lost:
\begin{itemize}
	\item at each step of the encoder and decoder paths,
	\item between the encoder parts and their corresponding decoder parts (skip connections) which ensures that the feature maps which were learned during the downsampling will be utilized in the reconstruction. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Several downsampling (Max-pool) blocks are used in the encoder section.
Each block applies two convolutional layers followed by a (\(2\times2\)) max pooling with a (\(2\times2\)) strides that selects the maximum value in a local pool filter in one feature map (or \(n\)-feature maps), resulting in a reduction in the dimension of feature maps~\cite{Lecun2015}, as a result, a reduction in computation complexity.
Each convolutional layer does \((3\times3)\) convolution operations, then batch normalization (BN), and finally a Relu.
Furthermore, after each downsampling block, the number of convolutional filters is increased, allowing the model to learn complex patterns successfully.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The bottleneck layer is a joining point in the model's deepest layer, located between the encoder and the decoder.
Two convolutional layers with \((1024)\) filters make up the bottleneck, which aids the model in learning and recognizing complex features.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The decoder is composed of a number upsampling blocks that function together to recover original input dimensions and improve resolution.
As in the downsampling block, each upsampling block transmits the input through two convolution layers, followed by a transmission up layer consisting of a transposed convolutional layer (upsampling).
The transposed convolutional layer varies from the standard upsampling function in that it introduces learnable parameters for the transposed convolution filters, which improve the model's learning process.
Furthermore, the number of filters used by the convolutional layer is reduced by half after each upsampling operation to keep the model symmetrical.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure4.png}
	\end{center}
	\caption{Res-UNet architecture.} 
	\label{fig:Unet}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{VGG16 encoder-decoder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The application of the VGG16~\cite{Simonyan2015} architecture as a backbone encoder to the UNet~\cite{Ronneberger2015} approach is addressed in this model.
VGG16 is a classification algorithm that consists of 13 convolutional layers, pooling layers, and (3) dense layers.
The dense layers were removed form the original VGG16 model, and a 13 convolutional layers were applied resulting in an encoder-decoder scheme for pixel-wise image segmentation.
The architecture of the VGG16 encoder-decoder model is shown in Fig.~\ref{vgg16}.
The model is U shaped like, and consists of two parts: encoder and decoder.
The encoder is made up of (five) convolutional blocks with a total of (13) \((3\times3)\) convolutional layers, followed by BN and Relu as the activation function.
After each convolutional block, a Max pool operation with a pool size of \((2\times2)\) is conducted, followed by dropout. 
The upsampling process is used to retrieve spatial resolution, and it contains \(5\) convolutional blocks of total \(13\) convolutional layers.
Bilinear interpolation with \((2\times2)\) kernel size is used for upsampling.
In order to improve recovering fine-grained information, skip connections were added between downsampling blocks and the matching upsampling blocks, allowing feature re-usability from earlier layers.
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure5.png}
	\end{center}
	\caption{VGG16 encoder decoder architecture.} 
	\label{vgg16}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{FCN-DenseNet model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
FCN-DenseNet is a pixel-wise image segmentation algorithm that was first introduced in~\cite{Jegou}.
To boost the resolution of the final feature map, FCN-DenseNet uses a U-shaped encoder-decoder architecture with skip connections between downsampling and upsampling channels.
Hence, FCN-DenseNet introduced a dense block representing its main component.
The dense block is made up of \(n\) layers, each of which is made up of a set of operations, as given in Table~\ref{layers}.
The purpose of the dense block is to concatenate the input (\(x\)) (feature maps) of a layer  with its output (feature maps) to emphasize spatial details information.
The architecture of the dense block is presented in Fig.~\ref{dense_block}. 
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0,angle=-90]{figure6.png}
	\end{center}
	\caption{Dense block architecture.} 
	\label{dense_block}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A transition down layer was added to execute a \((1\times 1)\) convolution followed by a \((2\times 2)\) Maxpooling operation to minimize the spatial dimensionality of the resulting feature maps.
As a result, a transition-up layer was added to recover the spatial resolution.
FCN-DenseNet essentially upsamples feature maps from the previous layer using a transpose convolution technique.
Upsampled feature maps are concatenated with those produced by the skip connection to provide the input to a new dense block.

As the upsampling approach expands the spatial resolution of the feature maps, the input to the dense block is not concatenated with its output during upsampling to avoid the overhead of memory shortage.
The FCN-DenseNet architecture for image segmentation utilized for delamination detection is shown in Fig.~\ref{fcn}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{FCN_dense_net.png}
	\end{center}
	\caption{FCN-DenseNet architecture.} 
	\label{fcn}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table~\ref{layers} presents the architecture of a single layer, the transition down  and transition up layers in details.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.3}
	\centering
	\scriptsize
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{ccccc}
			\hline
			Layer &  &  Transition Down &  &  Transition Up \\ 
			\hline
			Batch Normalization &  & Batch Normalization &  &  \(3\times 3\) Transposed Convolution  \\ 
			Relu &  & Relu &  & strides = (\(2\times2\))  \\ 
			(\(3\times3\)) Convolution &  & (\(1\times1\)) Convolution &  &  \\ 
			%		&  &   \\ 
			Dropout \(p=0.2\) &  &Dropout \(p=0.2\)  &  &  \\ 
			&  & (\(2\times2\)) Maxpooling &  &  \\ 
			\hline
		\end{tabular}
	}
	\caption{Layer, Transition Down and Transition Up layers.} 
	\label{layers}	
\end{table}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Pyramid Scene Parsing Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Pyramid Scene Parsing Network (PSPNet) was proposed by Zhao et al.~\cite{zhao2017pyramid} .
It is one of the most advanced semantic segmentation techniques. 
The principle idea of PSPNet is to provide adequate global contextual information for pixel-level scene parsing through concatenating the local and global features together. 
Hence, a spatial pyramid pooling module was introduced to perform four different pooling levels with four different pooling sizes and strides.
In this way, the pyramid pooling module is able to capture contextual features from different scales.

In this work, we implemented PSPNet with ResNet-50~\cite{He2016} as a backbone for feature map extraction with dilation at the last two layers of ResNet. 
Figure~\ref{fig:PSPNet} illustrates the implemented PSPNet architecture.
The pyramid pooling module was applied on four levels.
Global average pooling was used to produce the coarsest level of a single bin output shown in the red box. 
The other three sub-region levels have different pooling sizes of \((2\times 2), (4\times 4)\) and \((8\times8)\).
A \(1 \times 1\) convolutional layer was applied to the produced feature maps to reduce their dimensions, followed by a BN and Relu.
Then, the feature maps produced from the different levels were upsampled with bilinear interpolation.
Moreover, the upsampled features are concatenated with the output of the ResNet-50 model to obtain both local and global context information. 
Next, 2 convolutional layers were used for generating the pixel-wise segmented predictions. 
\begin{figure} [h!]
	\centering
	\includegraphics[scale=1.0]{figure7.png}
	\caption{PSPNet architecture.} 
	\label{fig:PSPNet}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Global Convolutional Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Global Convolutional Network (GCN) proposed by Peng et al. 
~\cite{Peng2017} addressed the importance to have large kernels for both 
localisation and classification for semantic segmentation to enlarge 
respective fields.
However, a contradiction arises when performing classification and localisation 
tasks. 
For instance, classification tasks require the models to be invariant for 
different transformations such as rotation and translation.
On the other hand, localisation tasks require the models to be sensitive for 
any transformation, to accurately assign each pixel for its semantic category.
Accordingly, to solve this contradiction, two design principles were suggested: 
\begin{enumerate}
	\item For the classification task, in order to improve the capability of 
	the model to handle different transformations, a large kernel size must be 
	used to enable dense connections between feature maps and per-pixel 
	classifiers.
	\item For localisation task, the model must be fully convolutional. 
	Additionally, fully connected or global pooling layers are not applied as 
	these layers will discard the localisation information. 
\end{enumerate}

Figure~\ref{fig:gcn} presents the proposed GCN module for semantic segmentation 
utilised for delamination identification.
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure8.png}
	\end{center}
	\caption{Global Convolution Network whole architecture.} 
	\label{fig:gcn}
\end{figure}
As shown in the Fig.~\ref{fig:gcn}, a residual network was utilised as a backbone for 
feature maps extraction, the residual block is shown in 
Fig.~\ref{fig:res_gcn_br}a.
After each residual block, a GCN block is inserted  
(Fig.~\ref{fig:res_gcn_br}b), which employs a combination of \((1\times 
k)\)+\((k\times 1)\) and \((k\times 1)\)+\((1\times k)\) convolutions which 
enables a dense connections within a large \((k\times k)\) region in the 
feature map.
In this work, we implemented the model with \(k=7\).
This is followed by a boundary refinement (BR) block shown in Fig.~\ref{fig:res_gcn_br}c, which can be considered as an additional residual block to refine the predictions near the object boundaries ended up generating a lower resolution score map. 
Furthermore, the upsampling operation is done recursively, it upsamples the low 
resolution score maps then concatenate it with a higher one to produce a new 
score maps.
The deconvolution operation is repeated until the original image size is 
obtained.
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{figure9.png}
	\end{center}
	\caption{(a) Residual block, (b) Global Convolution Network block, (c) 
		Boundary Refinement} 
	\label{fig:res_gcn_br}
\end{figure}