%% SECTION HEADER ////////////////////////////////////////////////////////////////////////////////
\section{DL models for delamination identification based on animation of full wavefield}
\label{sec44}
In this section, I utilised full wavefield frames of Lamb waves propagation in an end-to-end deep learning model to identify CFRP delamination instead of using RMS images as in the FCN models.
Accordingly, a many-to-one prediction scheme was adopted by employing convolutional-based recurrent neural networks (RNNs) to perform pixel-wise image segmentation.
A sequence of full wavefield frames is fed into the proposed deep learning models in order to identify the delamination.
Hence, in the segmentation problem, there are two classes: undamaged and damaged.
To the best of our knowledge, it is the first implementation of deep neural networks utilising Lamb wave propagation animations for damage imaging with semantic segmentation.
The proposed model showed excellent results in identifying the delamination in the numerically generated dataset, and it also showed their capability to generalise delamination identification in real world scenarios.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data preprocessing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Similar to the previous work~\cite{Ijjeh2021}, 475 cases were simulated, representing Lamb wave propagation and interaction with single delamination for each case. 

It should be underlined that the previous dataset contained the RMS of the full wavefield, representing wave energy spatial distribution in the form of images for each delamination case~\cite{Kudela2020d}.
On the other hand, the currently utilised dataset contains frames of propagating waves (512 frames for each delamination scenario).
The new dataset is available online~\cite{Kudela2021}.

As mentioned earlier, the dataset contains 475 different cases of delaminations, with 512 frames per case, producing a total number of 243,\,200 frames with a frame size of \((500\times500)\)~pixels representing the geometry of the specimen of size \((500\times500)\)~mm\(^{2}\).
Thus, using all frames in each case has high computational and memory costs.
Frames displaying the propagation of guided waves before interaction with the delamination have no features to be extracted (see Fig.~\ref{fig:Full_wave}).
Hence, for training, only a certain number of frames were selected from the initial occurrence of the interactions with the delamination.

Figure~\ref{fig:Full_wave} shows selected frames at different time-steps of the propagating Lamb waves before and after the interaction with the damage.
Frame \(f_{1}\) represents the initial interactions with the delamination, which was calculated using the delamination location and the velocity of the \(A0\) Lamb wave mode.
While frame \(f_{m}\) represents the last frame in the training sequence window, \(m=24\) for the developed model which will be discussed in the next subsection.
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{Figures/Chapter_4/figure2.png}
	\caption{Sample frames of full wave propagation.}
	\label{fig:Full_wave}
\end{figure}

Furthermore, the dataset was divided into two sets: training and testing, with a ratio of \(80\%\) and \(20\% \) respectively.
Moreover, a certain portion of the training set was preserved as a validation set to validate the model during the training process.
Additionally, the dataset was normalised to a range of \((0, 1)\) to improve the convergence of the gradient descent algorithm.

Additionally, for the training purposes, I have upsampled the frames (by using cubic interpolation) to \(512\times512\)~pixels to maintain the symmetrical shape during the encoding and decoding process.
Further, the validation sets have portions of \(10\%\) and \(20\%\) regarding the training set.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~\ref{fig:Diagram_exp_predictions} illustrates the complete procedure of obtaining intermediate predictions for the testing cases and finally calculating the RMS image, where \(f_{1}\) refers to the starting frame and \(f_{n}\) is the last frame, (\(n=512\)) in our dataset.
Further, \(m\) refers to the number of frames in the window, hence, \(m=24\) frames for the developed model, and \(k\) represents the total number of windows.
Accordingly, I slide the window over all input frames.
The shift of the window is one frame at a time.
Deep learning model predictions \(\hat{Y_k}\) are obtained for each window and combined to final damage map by using the RMS:

\begin{equation}
	RMS = \sqrt{\frac{1}{N}\sum_{k=1}^{N}\hat{Y_k}^2}.	
	\label{RMS}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{Figures/Chapter_4/figure3_diagram.png}
	\caption{The procedure of calculating the RMS prediction image (damage map).}
	\label{fig:Diagram_exp_predictions}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Autoencoder ConvLSTM model}
\label{proposed_approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this work, I developed an end-to-end deep learning model utilising full wavefield frames of Lamb wave propagation for delamination identification in CFRP materials as presented in Fig.~\ref{fig:proposed_convLSTM_model}.
The developed model has a scheme of many-to-one sequence prediction, which takes \(n\) number of frames representing the full wavefield propagation through time and their interaction with the delamination to extract damage features, and finally predict the delamination location, shape, and size in a single output image.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!h]
	\centering
	\includegraphics[width=5cm]{Figures/Chapter_4/figure3b.png}
	\caption{Autoencoder ConvLSTM model architecture.}
	\label{fig:proposed_convLSTM_model}
\end{figure} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the implemented model, I applied an autoencoder technique (AE) which is well-known for extracting spatial features.
The idea of AE is to compress the input data within the encoding process then learn how to reconstruct it back from the reduced encoded representation (latent space) to a representation that is as close to the original input as possible. 
Hence, I have investigated the use of AE to process a sequence of input frames to perform image segmentation.
Therefore, a Time Distributed layer presented in Fig.~\ref{fig:TD} was introduced to the model, in which it distributes the input frames into the AE layers in order to process them independently.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
	\centering
	\includegraphics[width=8cm]{Figures/Chapter_4/figure4_TD.png}
	\caption{Flow of input frames using Time distributed layer.}
	\label{fig:TD}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As previously mentioned, an AE consists of three parts: the encoder, the bottleneck, and the decoder.
The encoder is responsible for learning how to reduce the input dimensions and compress the input data into an encoded representation.

The encoder part presented in~\ref{fig:proposed_convLSTM_model} consists of four levels of downsampling. 
The purpose of having different scale levels is to extract feature maps from the input image at different scales.
Every level at the encoder consists of two 2D convolution operations followed by a Batch Normalization then a Dropout is applied. 
Furthermore, at the end of each level a Maxpooling operation is applied to reduce the dimensionality of the inputs. 

The bottleneck presented in Fig.~\ref{fig:proposed_convLSTM_model} has the lowest level of dimensions of the input data, further it consists of two 2D convolution operations followed by a Batch Normalization.

The decoder part presented in Fig.~\ref{fig:proposed_convLSTM_model}, is responsible for learning how to restore the original dimensions of the input.
The decoder part consists of two 2D convolutional operations followed by Batch Normalization and Dropout, and an upsampling operation is applied at the end of each decoder level to retrieve the dimensions of its inputs.
Skip connections linking the encoder with the corresponding decoder levels were added to enhance the features extraction process.
The outputs of the decoder were forwarded into the ConvLSTM2D layer to learn long-term spatiotemporal features.

Further, I applied a 2D convolutional layer as the final output layer followed by a sigmoid activation function which outputs values in a range from \((0,1)\) to indicate the delamination probability.
Consequently, a threshold value must be chosen to classify the output into a damaged represented by (\(1\)) or undamaged represented by (\(0\)).
Hence, I set the threshold value to (\(0.5\)) to exclude all values below the threshold by considering them as undamaged and taking only those values greater than the threshold to be considered as damaged.
\clearpage