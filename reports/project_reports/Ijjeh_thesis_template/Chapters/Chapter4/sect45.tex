%% SECTION HEADER ////////////////////////////////////////////////////////////////////////////////
\section{Super-Resolution image reconstruction for delamination identification}
\label{sec45}
%As previously stated, guided waves, specifically Lamb waves, are frequently used for SHM and NDT.
%For point-wise measurements in the former scenario, an array of transducers is typically used.
%These are typically piezoelectric transducers that can function as actuators as well as sensors, as in active guided wave-based SHM.
%It should be emphasized that round-robin actuator-sensor measurements can be performed very quickly, allowing for near-real-time monitoring of a structure.
%
%There has been a lot of recent research on the application of SLDV for NDT \cite{Flynn2013,Kudela2015,Kudela2018d,Segers2021,Segers2022}.
%For guided wave excitation, either a piezoelectric transducer or a pulse laser is utilized, and measurements are taken by SLDV at one location on the surface of an inspected structure.
%The method is continued automatically for other points in a scanning fashion until the full wavefield of Lamb waves is obtained.
%
%Full wavefield measurements are taken on a very dense grid of points opposite to sparsely measured signals by sensors.
%Hence, deliver much more useful data from which information about damage can be extracted in comparison to signals measured by an array of transducers.
%On the other hand, SLDV measurements take much more time than measurements conducted by an array of transducers.
%It makes the SLDV approach unsuitable for SHM in which continuous monitoring is required.
%But it is very capable for offline NDT applications.
%
%One can imagine that in a future matrix of laser heads instead of a single laser head used nowadays will be developed to reduce SLDV measurement time.
%Alternatively, compressive sensing (CS) and/or deep learning super-resolution (DLSR) can be applied.
%It means that SLDV measurements can be taken on a low-resolution grid of points and then full wavefield can be reconstructed at high-resolution.
%
%CS was originally proposed in the field of statistics~\cite{Candes2006,Donoho2006} and used for efficient acquisition and reconstruction of signals and images.
%It assumes that a signal or an image can be represented in a sparse form in another domain with appropriate bases (Fourier, cosine, wavelet).
%On such bases, many coefficients are close or equal to zero.
%The sparsity can be exploited to recover a signal or image from fewer samples than required by the Nyquistâ€“Shannon sampling theorem.
%However, there is no unique solution for the estimation of unmeasured data.
%Therefore, optimisation methods for solving under-determined systems of linear equations that promote sparsity are applied~\cite{Chen1998,VanEwoutBerg2008,VandenBerg2019}.
%Moreover, a suitable sampling strategy is required.
%
%Since then, CS has found applications in medical imaging~\cite{Lustig2007}, communication systems~\cite{Gao2018}, and seismology~\cite{Herrmann2012}.
%It is also considered in the field of guided waves and ultrasonic signal processing~\cite{Harley2013,Mesnil2016,Perelli2012,Perelli2015,DiIanni2015,KeshmiriEsfandabadi2018,Chang2020}
%
%Harley and Mura~\cite{Harley2013} utilised a general model for Lamb waves propagating in a plate structure (without defects) and $L_1$ optimisation strategies to recover their frequency-wavenumber representation. 
%They applied sparse recovery by basis pursuit and sparse wavenumber synthesis.
%They used a limited number of transducers and achieved a good correlation between the true and estimated responses across a wide range of frequencies.
%Mensil and Ruzzene~\cite{Mesnil2016} were focused on the reconstruction of wavefield that includes the interaction of Lamb waves with delamination.
%Similar to previous studies, analytic solutions were utilised to create a compressive sensing matrix.
%However, the limitation of these methods is that dispersion curves of Lamb waves propagating in the analysed plate have to be known a priori.
%
%Perelli et al.~\cite{Perelli2012} incorporated the warped frequency transform into a compressive sensing framework for improved damage localisation.
%The wavelet packet transform and frequency warping was used in~\cite{Perelli2015} to generate a sparse decomposition of the acquired dispersive signal.
%
%Di Ianni et al.~\cite{DiIanni2015} investigated various bases in compressive sensing to reduce the acquisition time of SLDV measurements.
%Similarly, a damage detection and localisation technique based on a compressive sensing algorithm was presented in~\cite{KeshmiriEsfandabadi2018}.
%The authors have shown that the acquisition time can be reduced significantly without losing detection accuracy.
%
%Another application of compressive sensing was reported in~\cite{Chang2020}. 
%The authors used signals registered by an array of sensors for tomography of corrosion.
%They investigated the reconstruction success rate depending on the number of actuator-sensor paths.
%
%The group of DLSR methods is applied mostly to images~\cite{Dahl2017,Zhang2018,Wang2019} and videos~\cite{Zhang2017,Yan2019}.
%Image super-resolution (SR) is the process of recovering high-resolution images from low-resolution images.
%A similar approach can be used in videos where data is treated as a sequence of images.
%Notable applications are medical imaging, satellite imaging, surveillance and security, astronomical imaging, amongst others.
%Also deep learning super sampling developed by Nvidia and FidelityFX super-resolution developed by AMD was adopted for video games~\cite{Claypool2006}.
%Mostly supervised techniques are employed
%which benefit from recent advancements in deep learning methods ranging from enhanced convolutional neural networks (CNN)~\cite{Zhang2017}, through an extension of PixelCNN~\cite{Dahl2017} to generative adversarial networks (GANs)~\cite{Wang2019}, to name a few.
%Nevertheless, so far neither of these methods have been applied to wavefields of propagating Lamb waves.
%The exception is an enhancement of wavefields as the second step of SR followed by classic CS~\cite{Park2017a,KeshmiriEsfandabadi2020}.

In this section, I present a framework for full wavefield reconstruction of propagating Lamb waves from spatially sparse SLDV measurements of resolution below the Nyquist wavelength $\lambda_N$. 
The Nyquist wavelength is the shortest spatial wavelength that can be accurately recovered from wavefield by sequential observations with spacing $\Delta x$ which is defined as $\lambda_N = 2 \Delta x$. 

For the first time, an end-to-end approach for the SR problem is used in which a deep learning neural network is trained on a synthetic dataset and tested on experimental data acquired by SLDV.
It means that the approach is solely based on DLSR.
It is different from methods presented in the literature which utilize CS theory~\cite{Harley2013, KeshmiriEsfandabadi2018} or CS theory in conjunction with super-resolution convolutional neural networks for wavefield image enhancement~\cite{Park2017a, KeshmiriEsfandabadi2020}.
The efficacy of the developed framework is presented and compared with the conventional CS approach.
The performance of the proposed technique is validated by an experiment performed on a plate made of carbon fibre reinforced polymer (CFRP) with embedded Teflon inserts simulating delaminations.

\subsection{Dataset preparation}
\label{sec451}
To train a deep learning model to perform super-resolution image reconstruction, I have to reproduce a low-resolution training set from the original high-resolution dataset.
Initially, I resized the frames in the original high-resolution dataset to \((512\times512)\) pixels to obtain the desired output frame shape while performing image reconstruction from the low-to high-resolution.


The maximum permissible distance between grid points according to Nyquist theorem is calculated as in Eqn.~(\ref{eq:dx}):
\begin{equation}
	d_{max}= \frac{1}{2*k_{max}} = \frac{1}{2*51.28\ [\textup{m}]} = \frac{\lambda}{2} = \frac{19.5}{2}\ \textup{[mm]}.
	\label{eq:dx}	
\end{equation} 
where $k_{max}$ is the maximum wavenumber, and $\lambda$ is the shortest wavelength.

On the other hand, the longest distance between grid points on uniform square grid in 2D space is along the diagonal as shown in Fig.~\ref{fig:Nyquist}.
Therefore, the number of Nyquist sampling points along edges of the plate is defined as:
\begin{equation}
	\begin{align}
	N_x= \frac{L}{d_{max}/\sqrt{2}}, \\
	N_y=  \frac{W}{d_{max}/\sqrt{2}},
	\end{align}
	\label{eq:Nyq}
\end{equation}
where $L$ is the plate length, and $W$ is the plate width.

In our particular case, $L=W=500$~[mm], and number of Nyquist points $N_x= N_y= N_{Nyq} =73$.

\begin{figure} [!h]
	\centering
	\includegraphics[scale=1]{Figures/Chapter_4/Nyquist_wavelength.png}
	\caption{Longest distance between grid points.}
	\label{fig:Nyquist}
\end{figure}


To train the DLSR model, I have generated low-resolution full wavefield frames of \((n\times n)\) points, where $n=32$, which is below the Nyquist sampling rate of a 2D frame.
Hence, I have performed image subsampling with bi-cubic interpolation and a uniform mesh of size \((32\times32)\) pixels with a compression rate (CR) of \(19.2\%\) from the Nyquist sampling rate as depicted in Eqn.~(\ref{CR}):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	CR(\%) = (\frac{n}{N_{Nyq}})^2 = (\frac{32}{73})^2=19.2\%
	\label{CR}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~\ref{fig:SR_LR} shows a three SR Frames with their corresponding LR frames at different time steps.

Furthermore, I selected \((128)\) consecutive full wavefield frames per delamination case to reduce computation complexity during training.
The selected frames start from the initial occurrence of the interaction with the delamination, as frames displaying the propagation of guided waves before interacting with the delamination have no valuable features to be extracted.

\begin{figure} [!h]
	\centering
	\begin{subfigure}[b]{.48\textwidth}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_4/SR_case_1_frame_1.png}
		\caption{HR Frame}
		\label{fig:SR_1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{.48\textwidth}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_4/LR_case_1_frame_1.png}
		\caption{LR frame}
		\label{fig:LR_1}	
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{.48\textwidth}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_4/SR_case_1_frame_63.png}
		\caption{HR frame}
		\label{fig:SR_2}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{.48\textwidth}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_4/LR_case_1_frame_63.png}
		\caption{LR frame}
		\label{fig:LR_2}	
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{.48\textwidth}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_4/SR_case_1_frame_128.png}
		\caption{HR frame}
		\label{fig:SR_3}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{.48\textwidth}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_4/LR_case_1_frame_128.png}
		\caption{LR frame}
		\label{fig:LR_3}	
	\end{subfigure}
	\caption{High-resolution (HR) and Low-resolution (LR) frames at different time steps.}
	\label{fig:SR_LR}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsection{DL approach for SR image reconstruction}
\label{sec63}
Single image Super-Resolution (SISR) aims to generate a visually pleasing high-resolu\-tion (HR) image from its de-graded low-resolution (LR) measurement.

%\subsection{Residual Dense Network model}
Residual dense network (RDN) was introduced by Zhang et al.~\cite{Zhang2018} to perform SISR.
RDN aims to solve the issue of unexploited hierarchical features obtained from the original low-resolution (LR) images.
Accordingly, to resolve this issue RND introduced a residual dense block (RDB) which is capable to fully exploit all hierarchical features obtained from all convolutional layers.

Figure~\ref{fig:RDB} shows the architecture of a RDB which consists of four  layers (\(L_1,\ L_2,\ L_3,\- \ L_4\)).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{Figures/Chapter_4/RDB.png}
	\end{center}
	\caption{Residual Dense Block architecture.} 
	\label{fig:RDB}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Therefore, a RDB can extract the abundant local features through dense connected convolutional layers leading to a local residual learning.
The local feature fusion within each RDB is utilised to learn more useful features from the previous and current local features, therefore, stabilising the training process as the network depth increases.
Consequently, RDB enables direct links from the previous RDB to all layers of the current RDB, resulting in a contiguous memory (CM) mechanism.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this work, the implemented deep learning model was inspired by the RDN~\cite{Zhang2018}. 
The model architecture is presented in Fig.~\ref{fig:RDN}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{Figures/Chapter_4/RDN.png}
	\end{center}
	\caption{Implemented Residual Dense Network architecture.} 
	\label{fig:RDN}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The first segment in the model is the Shallow Feature Extraction Net (SFENet) which consists of two cascaded convolutional layers responsible for extracting shallow features from the original LR input.
Then, the extracted features from SFENet are transferred to the segment of RDBs in which two RDBs were utilised.

The third segment is the Dense Feature Fusion (DFF) which is responsible for fusing features that include global feature fusion and global residual learning.
The purpose of global feature fusion is to learn global hierarchical features holistically.
Hence, DFF fully utilise all features from all preceding segments.

The last segment in the model is the Up-Sampling Net (UPNet), in which I applied the pixel shuffle technique~\cite{Shi2016}.
Further, the pixel shuffle performs sub-pixel convolution operation that is responsible to reshape its input tensor by rearranging the elements \((H\times W\times r^2)\) to \((rH\times rW\times 1)\), where \(H\) is the height, \(W\) is the width, \((r^2)\) is total number of channels, and \(r\) is the up-scaling factor.
Accordingly,Â the number of channels at the last layer (output from DFF segment)Â must equal \(C.r^2\) for the total number of pixels in order to match the HR image to be obtained.
Hence, the up-scaling factor \(r\) equals to \(16\), as our aim is to obtain HR output image of size \((512\times 512)\) from the LR input image of size \((32\times 32)\).
Figure~\ref{fig:sub_pixel_layer} illustrates the process of the sub-pixel convolution layer as it is made up of two steps: a general convolutional operation and pixel rearrangement.Â 
Further, it works through combining each pixel on multiple-channel feature maps into one \((r\times r)\) square area in the output image. 
Therefore, each pixel on feature maps is equivalent to the sub-pixel on the generated output image.
The final convolutional layer has \(1\) filter of size \((1\times 1)\), which will produce \(1\) output channel as the HR images are in grayscale. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[scale=1.0]{Figures/Chapter_4/sub_pixel_convolution.png}
	\end{center}
	\caption{Sub-pixel convolution layer.} 
	\label{fig:sub_pixel_layer}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Furthermore, Fig.~\ref{fig:dlsr_loss} presents the plot of the training loss and validation loss values with respect to epochs during the training phase for the DLSR model.
The number of epochs for the DLSR model was set to $250$.
As shown in the Fig.~\ref{fig:dlsr_loss}, the developed AE-ConvLSTM model converges, and it has no signs of overfitting or underfitting.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [h!]
	\begin{center}
		\includegraphics[width=.85\textwidth]{Figures/Chapter_4/dlsr_loss.png}
	\end{center}
	\caption{DLSR model: training and validation losses during training phase.} 
	\label{fig:dlsr_loss}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
