%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Machine learning approach}
\label{sec31}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Machine learning (ML) is a sub-field of AI that belongs to the computer science field.
ML is defined as \enquote{the ability of a computer to learn without being explicitly programmed}~\cite{munoz2014machine}.

The conventional way of software engineering is through creating rules by humans and then combining them with data to find a solution to a problem.
Alternatively, when it comes to ML, it utilises data and answers to learn the rules behind the problem~\cite{franoischollet2017learning}.
In Fig.~\ref{fig:Machine_learning} the conventional software programming and ML are presented in (a) and (b) respectively.
In ML, machines have to run through a learning process to learn inference rules, which are responsible for controlling the relations within a phenomenon. 
Hence, it is called an ML.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!ht]
	\begin{center}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_1/machine_learning_vs_conventional_programming.png}
	\end{center}
	\caption{(a) Conventional Programming	(b) Machine learning.} 
	\label{fig:Machine_learning}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

ML techniques in SHM were heavily utilised by researchers for damage detection~\cite{raghavan2008effects, Su2009, Mitra2016}.
Moreover, ML techniques attempt to map the patterns of the input data acquired by sensors to output targets for damage estimation at different levels.
Accordingly, ML techniques demand high domain knowledge of the examiner to perform hand-crafted damage-sensitive feature extraction on the raw data acquired by sensors before being fed into a suitable ML model.
Generally, the process of damage-sensitive features extraction (hand-crafted) in the field of SHM emerged due to the enormous development in the physics-based SHM techniques such as modal strain energy (MSE)~\cite{Kim}, modal curvature (MC)~\cite{Wahab}, modal assurance criterion (MAC), and coordinate (MAC)~\cite{Allemang2003}, modal flexibility (MF)~\cite{Jaishi}, damage locating vector (DLV)~\cite{Bernal2002}, wavelet transform \cite{Staszewski, Kima} and probabilistic reconstruction algorithm (PRA)~\cite{Hay2006} among others.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Different methods can be implemented when performing ML.
Generally, those methods are grouped into four approaches: supervised learning, unsupervised learning, reinforce\-ment learning, and transfer learning.
Fig.~\ref{fig:Machine_learning_approaches} shows the different types of ML approaches.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!ht]
	\begin{center}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_1/ML_approaches.png}
	\end{center}
	\caption{Machine Learning Approaches.} 
	\label{fig:Machine_learning_approaches}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Supervised learning is the task of training a machine to learn how to develop inference rules from the training data and how to map inputs with outputs.
The training data is a collection of variables together with their labels (e.g., a set of civil images) of structures that are labelled as damaged or undamaged (healthy).
During the learning process, the machine gets a collection of inputs simultaneously with the corresponding label (ground truth).
Accordingly, by comparing its predicted output with the correct output to find errors, it modifies the model and the learning occurs~\cite{Ongsulee2018}. 
Supervised learning uses patterns to predict the values of the output label for new unlabeled data by applying methods like regression and classification~\cite{Ongsulee2018}. 
%Fig~\ref{fig:Machine_learning_approaches} presents most utilised algorithms for ML different approaches like: 
%K Nearest Neighbors algorithm (KNN) where K represents the number of the nearest neighbours used for classification of the observations in a test sample, based on their characteristics e.g. the mean distance. 
%Moreover,Decision Trees where the data keeps splitting according to a specific parameter. 
%Furthermore, Naive Bayes algorithm which is based on probabilistic approach, through implementing Bayes' theorem.
%In addition, Support vector Machine SVM and Logistic regression. 
%For Regression purposes, algorithms like linear and polynomial regression are implemented.

Unsupervised Learning is applied to such data with no historical labels~\cite{Ongsulee2018}. 
In this case, the model does not know the ground truth labels of the input values. Therefore, the algorithm needs to figure out some common characteristics among the input values.
Consequently, unsupervised learning is more difficult than supervised learning due to the absence of supervision, which implies the problem becomes less defined.
%The most well-known techniques used in Unsupervised learning is clustering~\cite{Russell2010}. 
%In which it creates subgroups within the input data based on their characteristics, Fig~\ref{fig:Machine_learning_approaches} presents most Clustering algorithms like:k-means algorithm, which intends to split n observations into k clusters, where each observation relates to the cluster that has the nearest mean distance.   

Reinforcement learning is based on the trial and error principle, which means the algorithm learns through actions that explore the environment in a way that results in the greatest rewards~\cite{Russell2010}.
The reinforcement learning process consists of three parts: the agent, which is responsible for making decisions; the environment that relates to any interaction with the agent; and the actions that are taken by the agent. 
Figure.~\ref{fig:ReinforcementLearning} illustrates the procedure of the reinforcement learning approach.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!ht]
	\begin{center}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_1/Reinforcement_learning.png}
	\end{center}
	\caption{Reinforcement Learning.} 
	\label{fig:ReinforcementLearning}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Transfer learning differs in a way compared to the traditional ML approaches designed for particular tasks, implying that their learning and knowledge can not be transferred from one model to another.
Therefore, when starting a new ML task, we have to start from scratch.
On the contrary, in transfer learning, the model knowledge (e.g., features and weights) can be transferred from a previously learned task to a new learning task.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection[Data prepossessing and FE]{Data prepossessing and feature extraction techniques}		

In SHM applications, the damage identification process is based on comparing the collected data from the structure without damage (base-line) with its current status to determine if there are any occurrences of changes such as damage.
Accordingly, signal processing techniques must be applied to the collected data to identify components of interest in a registered signal from a structure.
In general, the process of extracting features of the defects that occurred in structures can be achieved in different domains: time domain, frequency domain, time-frequency domain, impedance domain, and modal analysis domain~\cite{Khan2019}.

In this section, various methods for signal processing, data prepossessing and feature extraction are presented.

\subsubsection{Fourier Transform} 
The Fourier Transform (TF) is considered a conventional method for signal analysis.
It is used to decompose the registered time-domain signal into its frequency components.
Then, the signal can be analysed for its frequency components because the FT coefficien\-ts of the transformed function demonstrate the contribution of the sine and cosine functions at each frequency.
FT presents global information about the frequency content; therefore, it is suited for signals with stationary frequency content, meaning their frequency content does not change with time~\cite{Raghavan2006}.
Alternatively, there are other time-frequency representations (TFR) that can identify the local frequency content and are better suited for non-stationary-frequency signals~\cite{Raghavan2006}.
The Short-time Fourier transform (STFT) is considered the simplest example of a TER, in which the STFT divides the signal into a number of short overlapping segments in the time domain.
Each segment is multiplied in time using a fixed modulation window and the FT is used on the resulting signal~\cite{Raghavan2006}.

\subsubsection{Wavelet Transform} 
The Wavelet Transform (WT) is a mathematical function for data preprocessing that enhan\-ces the process of feature extraction in a wide range of applications such as civil engineering, power engineering, traffic engineering, mechanical systems, and aerospace engineering.
Furthermore, WT is considered one of the most widely used tools for signal preproces\-sing in SHM in recent years~\cite{Taha2006}.
The principal idea of WT is to split data signals into different scale components, accordingly, analysing each component with a resolution matched to its scale~\cite{Graps1995}.
The WT is based on dilated scales and shifted windows that can perform a time-frequency resolution of a data signal. 
WT is represented in the following Eqn.~(\ref{wavelet}), which yields a 2D coefficients matrix  $WT\{x\}(a,b)$:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	WT\{x\}(a,b) = \int_{R}^{}\Psi_{a,b}(t)x(t)dt,
	\label{wavelet}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
$\Psi_{a,b}$ is defined as the mother wavelet which scales and dilates wavelets, where $a$ and $b$ are the scales and dilation parameters.
Scaling in WT indicates stretching or compressing it in the time domain. 
Therefore, compressed wavelets are represented by smaller scales while stretched wavelets can be produced by larger scales~\cite{Graps1995}.
%\subsubsection{Principle component analysis} Principle component analysis (PCA) is a technique of multi-variable and mega-variate analysis used for reducing complex data dimensionality in ML. 
%Furthermore, PCA can be identified as an unsupervised, simple and non-parametric method for information extraction and data compression~\cite{Jolliffe2002}.
%
%Consequently, PCA is considered as a patterns recognition technique, and when it is applied on collected data, new important hidden data with some simplified patterns  are identified.
%Accordingly, PCA is responsible for determining the dynamics in the system according to their importance, as a result, there are more important dynamics and redundant dynamics and which are just noise~\cite{Farrar2007}.
%To develop a PCA model it is essential to organise the data in an (\(m \times n\)) matrix \(X = [x_{i1}x_{i2}...x_{ij}]\) where $i = 1,2,3...m ; j = 1,2,3,...n$ which carries information from \(n\) sensors (variables) and \(m\) experimental trials (observations).
%Considering different magnitudes and scales regarding the physical variables and sensors in the structure, each point in the collected data is computed using the mean of all the sensor measurements at the same time and the standard deviation of all sensor measurements.
%Following normalization the variables the covariance matrix $C_x$ is calculated as show in  Eqn~\ref{covar matrix}~\cite{Tibaduiza}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{equation}
%	C_x =  \frac{1}{m-1}X^TX
%	\label{covar matrix}
%\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%where $C_x$ is a square symmetric $(m \times m)$ matrix that determines the linear relationship degree in the data set within all possible pairs of variables which are the sensors in this case, and $T$ is the transposition.
%Considering the covariance matrix $C_x$ and the eigenvalues $(\lambda) $ of $C_x$, therefore, the eigenvector $(E)$ can be determined according to Eqn~\ref{eigenvector}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{equation}
%	C_xE=\lambda E
%	\label{eigenvector}
%\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Columns of the eigenvectors matrix $E$ are arranged based on the eigenvalues by descending order and they are termed the Principal Components (PCs) of the data set.
%Accordingly, the most important features in the data with the highest weight of information are represented by the eigenvectors with the highest eigenvalue.
%Therefore, by picking only a reduced number of $r$ of PCs, that is corresponding to the first eigenvalues, the reduced transformation matrix could be considered as a model for the structure with compressed data. 
%
%The transformed data matrix T (score matrix) can be represented geometrically as the projection of the original data over the direction of the PCs of the eigenvector matrix E as presented in Eqn~\ref{score matrix}.
%The Principal Component Coefficient (PCC) quantify the influence of each variable $(x_{1,i},x_{2,i},x_{3,i},...,x_{i,j})$ have on each principle component $(z_{i,1},z_{i,2},z_{i,3},...,z_{i,j})$.
%For the PCC matrix $W$, the rows represent the variables, columns represent the component the PCC for each variable mentioned before, the component principal can be calculated as shown in Eqn~\ref{PCC},
%where $e_{i,j}$ denotes an element of eigenvector matrix E and Var($x_{i,j}$) denotes the variance of $x_{i,j}$~\cite{DeOliveira2014}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{equation}
%	T = XE
%	\label{score matrix}
%\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{equation}
%	W = \frac{e_{i,j}}{\sqrt{ Var(x_{i,j})}}
%	\label{PCC}
%\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table}
%	\renewcommand{\arraystretch}{1.1}
%	\centering
%	\caption{Advantages/Disadvantages of PCA}
%	\scriptsize	
%	\begin{tabular}{ll} 
%		\toprule
%		\textbf{Advantages} & \textbf{Disadvantages} \\ 
%		\midrule
%		Removes Correlated Features & Independent variables become less 	interpretable  \\ 
%		%\hline
%		Improves Algorithm Performance & Data must be standardized before PCA \\ 
%		%	\hline
%		Reduces Overfitting & Information Loss \\
%		%	\hline
%		Improves Visualisation &  \\
%		\bottomrule
%	\end{tabular}
%	\label{tab:pca pros and cons}
%\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Consequently, determining the optimal number of PCs is performed by looking at the cumulative variance ratio as a function of the number of components. The choice of selecting the number of PCs completely relies on the trade-off between information loss and dimensionality reduction. 
%PCA technique has several advantages and disadvantages, as presented in Table~\ref{tab:pca pros and cons}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Principal component analysis}
PCA is a popular method used for damage identification in SHM.
Further, PCA shows a solid and efficient performance in feature extraction and structural damage detection~\cite{liu2014research, wang2014principal, nguyen2010fault}.
Besides, PCA proves to be an effective tool to improve the training efficiency and enhance the classification accuracy for other ML algorithms, such as unsupervised learning methods~\cite{liu2019rapid, datteo2017statistical, torres2014data}.
	
PCA is a dimensionality reduction technique utilised to reduce the dimensionality of large data (input space) into a lower dimension (feature space) through transforming a large set of variables into a smaller one with minimal loss information~\cite{Jolliffe2002}.
Moreover, PCA can be used for damage detection by eliminating noise and obtaining sensitive features of damage as eigenvectors.
The PCA technique is illustrated below.
In the beginning, a matrix \(U(t)\) is constructed as shown in Eqn.~(\ref{U(t)}), which contains all registered data with time histories:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	U(t)=
	\begin{bmatrix}
		u_1{(t1)}       & u_2{(t1)} & \dots & u_M{(t1)} \\
		u_1{(t2)}       & u_2{(t2)} & \dots & u_M{(t2)} \\
		\vdots 			& \vdots 	& \ddots & \vdots \\
		u_1{(t_N)}      & u_2{(t_N)} & \dots & u_M{(t_N)}
	\end{bmatrix}\ .
	\label{U(t)}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where \(t\) corresponds to the time, \(u_i\ (i = 1, 2, ..., M)\) represents the response from the \(i-th\) sensor installed in the monitored structure, \(M\) represents the total number of sensors, \(t_j\ (j = 1, 2, ..., N)\) represents the \(j-th\) time step of the data, and \(N\) is the total time observations during monitoring.
Additionally, each column represents the data registration of one sensor.
The next step is to normalise the time series of each sensor data registration by subtracting the mean value shown in Eqn.~(\ref{mean value}):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	\bar{u_i} = \frac{1}{N}\sum_{j=1}^{N}u_i(t_j)\ .
	\label{mean value}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Equation~(\ref{normalised matrix}) represents the normalised matrix:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	U'(t)=
	\begin{bmatrix}
		u_1{(t1)}-\bar{u_1}       & u_2{(t1)}-\bar{u_2} & \dots  & u_M{(t1)}-\bar{u_M} \\
		u_1{(t2)}-\bar{u_1}       & u_2{(t2)}-\bar{u_2} & \dots  & u_M{(t2)}-\bar{u_M} \\
		\vdots 					  & \vdots 	  			& \ddots & \vdots \\
		u_1{(t_N)}-\bar{u_1}      & u_2{(t_N)}-\bar{u_2}& \dots  & u_M{(t_N)}-\bar{u_M}
	\end{bmatrix}
	\label{normalised matrix}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
After computing the normalised matrix, the covariance matrix is computed as shown in Eqn.~(\ref{covariance}):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	C = \frac{1}{M}U'^TU' \ .
	\label{covariance}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Next, the eigenvalue and the corresponding eigenvector of the covariance matrix are computed through solving the following Eqn.~\ref{eigvalue}:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	(C-\lambda_iI)\psi_i =0 \ .
	\label{eigvalue}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where \(I\) represents the \(M\times M\) identity matrix, \(\psi_i = [\psi_{i,1},\psi_{i,2}, \hdots, \psi_{i,j}]^T\) in which \(\psi_{i,j}(j=1, 2, \hdots, M)\) is the element related to the \(j-th\) sensor.
Usually, eigenvalues are sorted into decreasing order, particularly \(\lambda_1>\lambda_2>\hdots>\lambda_M\). 
Then, the first eigenvector \(\psi_1\) corresponding to \(\lambda_1\) holds the greatest variance and consequently holds the most important information for the original matrix $U$.
The first few principal components hold most of the variance, whereas the remaining less important components involve the measurement of noise.
Accordingly, the first few eigenvectors are utilised as sensitive features for damage detection and localisation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Auto-associative Neural Networks}
Auto-associative Neural Network (AANN), which is also called an autoencoder, is considered as one of the ANN architectures.
Generally, AANN is composed of five layers as shown in the Fig.~\ref{fig:AANN}, which includes the input layer, mapping layer, bottleneck layer (has fewer neurons than the input and the output layers), demapping layer, and the output layer.
AANN is considered an unsupervised learning technique.
The idea behind AANN is to map the input using nonlinear functions and then reconstruct it using nonlinear functions so the network can learn from the inputs themselves.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!ht]
	\begin{center}
		\centering
		\includegraphics[scale=1]{Figures/Chapter_1/Auto-associative NN.png}
	\end{center}
	\caption{Auto-associative Neural Network architecture.} 
	\label{fig:AANN}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Originally, the AANN technique was based on nonlinear principal component analysis (NLPCA), which is a powerful statistical technique used in the process of feature extraction and data dimensionality reduction~\cite{Dervilis2014}. 
The difference between the PCA and NLPCA is that NLPCA is the utilisation of nonlinear functions for mapping the input data as shown in Eqn~\ref{NLPCA}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	T= G(X) .
	\label{NLPCA}
\end{equation} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As mentioned previously, $T$ is the score matrix, $X$ is the input data of size $(m \times n)$ where $m$ represents the number of variables and $n$ represents the number of observations, and $G$ is a nonlinear vector function that holds several individual nonlinear functions.
Accordingly, the de-mapping process is performed by the inverse of the Eqn~(\ref{NLPCA}) using a nonlinear function $H$ as shown in Eqn~(\ref{inverseNLPCA}).
The loss of information that occurred due to the mapping and de-mapping process can be calculated in the reconstruction error matrix as shown in Eqn~(\ref{errorMatrix})~\cite{Dervilis2014}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	\hat{X} = H(T)
	\label{inverseNLPCA}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
	E= X-\hat{X}
	\label{errorMatrix}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Mahalanobis squared distance}
MSD is an effective multivariate distance measuring technique in which it measures the distance between a point and a distribution.
Therefore, MSD is utilised with multivariate statistics for outlier detection~\cite{Worden2000}.
Assuming \(X\) to be training set with data acquired when the undamaged structure is under environmental and/or operational variations (EOVs) with multivariate mean vector \(\mu\) and covariance matrix \(\Sigma\)~\cite{Farrar2013}.
Accordingly, the damage index \((DI_i)\) between feature vectors from the training set \(X\) and any new feature vector from the test matrix \(Z\) is calculated using Eqn.~(\ref{msd}):
\begin{equation}
	DI_i = (z_i-\mu)\Sigma^{-1}(z_i-\mu)^T,
	\label{msd}
\end{equation}
where \(z_i\) is a tested feature vector.
The performance of this technique mainly relies on acquiring all likely EOVs in the training set~
\cite{Farrar2013}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Gaussian mixture models}
Gaussian mixture models (GMM) are a clustering method commonly used with unsuper\-vised learning, in which it aims to find main clusters of points in a dataset that share some common characteristics or features.
Additionally, GMM has also been referred to as Expectation-Maximization (EM) clustering and is based on the optimization strategy.
The damage detection is performed based on multiple MSD-based algorithms, in which the covariance matrices and mean vectors are functions of the main components.
A GMM is defined as a superposition of \(K\) Gaussian distributions as shown in Eqn.~(\ref{gmm}):
\begin{equation}
	p(x) = \sum_{k=1}^K P(k) \mathcal{N}(x|\mu_k,\Sigma_k),
	\label{gmm}
\end{equation}
where \(x\) represents the training samples in the dataset, and \(P(k)\) corresponds to the mixture proportion (contribution weight) of the \(k-\)th distribution, in which the mixture proportion must satisfy \(0\leq P(x)\leq 1\).
The sum of all mixture proportions satisfies the following Eqn.~(\ref{mixture}):
\begin{equation}
	\sum_{k=1}^{K}P(x) =1.
	\label{mixture}
\end{equation}  
\(\mathcal{N}(x|\mu_k,\Sigma_k)\) refers to the conditional probability of the instance \(x\) for the \(k-\)th Gaussian distribution \(\mathcal{N}(\mu_k,\Sigma_k)\) presented in Eqn.~\ref{conditional}:
\begin{equation}
	\mathcal{N}(x|\mu_k,\Sigma_k) = \frac{\exp(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k))}{(2\pi)^{\frac{d}{2}\sqrt{\det(\Sigma_k)}}},
	\label{conditional}		
\end{equation}
where \(\mu_k\) and \(\Sigma_k\) are the mean and the covariance of that Gaussian distribution, respectively.
The complete GMM is parameterized by the mean vectors, covariance matrices and the mixture weights from all component densities \(\{\mu_k,\Sigma_k, P(x)\}_{k=1,\hdots,K}\).

The parameters can be carried out from the training data using the classical maximum likelihood estimator (CMLE) based on the EM algorithm~\cite{Dempster1977}.
Damage can be detected through estimating \(k\) \(DIs\) for each data sample \(x\) as shown in Eqn.~(\ref{DIs}):
\begin{equation}
	DI_q(x) = (x-\mu_k)\Sigma_k^{-1}(x-\mu_k)^T,
	\label{DIs}
\end{equation}
where \(\mu_k\) and \(\Sigma_k\) refers to all observations from the \(k\) data component.
For each observation the \(DI\) is given by the smallest \(DI\) estimated on each component as in Eqn.~(\ref{DI}):
\begin{equation}
	DI(x) = \min[DI_k(x)],
	\label{DI}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classification techniques}
In this section, various classification techniques utilised in classifying the extracted features are presented.
\subsubsection{Support vector machine}
Support vector machine (SVM) is a supervised ML model that is utilised as a classifica\-tion and regression tool.
The idea behind SVM is to find an optimal hyperplane (e.g separate line) in N-dimensional space (N is the number of features) that separates the classes. 
Furthermore, the hyperplane aims to maximize the margin between the points on either side, hence the so-called \enquote{decision line/boundary}. 
Also, when we try to separate two classes of data points, we could have many possible hyperplanes. 
However, our goal is to find the hyperplane that has the maximum margin (i.e., the maximum distance between data points of both classes).
Figure~\ref{fig:SVM} shows SVM hyperplanes in 2D feature space and 3D feature space.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!ht]
	\centering
	\begin{subfigure}[b]{0.40\textwidth}		
		\includegraphics[width=1\textwidth]{Figures/Chapter_2/2d_svm.png}
		\caption{Hyperplane 2D feature space.}
		\label{fig:2dsvm}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=1\textwidth]{Figures/Chapter_2/3d_svm.png}
		\caption{Hyperplane 3D feature space.} 
		\label{fig:3dsvm}
	\end{subfigure}	
	\caption{SVM for 2D and 3D feature space.}
	\label{fig:SVM}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{K-Nearest Neighbor}
K-Nearest Neighbor (KNN) is a supervised ML technique utilized to perform classifica\-tion tasks.
KNN does not have a specialized training phase.
It saves all the training data and uses the entire training set for classifying a new data point, which adds time complexity at the testing time.
Moreover, KNN is a non-parametric learning algorithm, which means it does not make any assumptions regarding the input data, which is useful considering the real-world data does not obey the typical theoretical assumptions such as linear separability, and uniform distribution, among others.

In the KNN technique, first, the distance between the new data point and the other data points is calculated.
Furthermore, any distance method can be applied, e.g., Euclidean, Manhattan, etc.
Accordingly, it picks the K-nearest points, where K is an integer number (number of neighbours) that can be chosen in such a way that the model will be able to predict new unseen data accurately.
Then, it assigns the new data point to the class to which the majority of the K data points belong.
Figure~\ref{fig:datapoints} shows initial data points (training set) before classification, and Fig.~\ref{fig:KNN_K_5} shows the result of applying KNN techniques to the data points (3 classes) assuming \(K=6\).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!ht]
	\centering
	\begin{subfigure}{0.49\textwidth}		
		\centering
		\includegraphics[width=1\textwidth]{Figures/Chapter_2/KNN_data_points.png}
		\caption{Data points.} 
		\label{fig:datapoints}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{Figures/Chapter_2/KNN_K_6.png}
		\caption{3-Classes with \(K=6\).} 
		\label{fig:KNN_K_5}
	\end{subfigure}	
	\caption{KNN algorithm: data classification with \(K=6\).}
	\label{fig:KNN}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Decision tree}
Decision trees are supervised ML algorithms that are used in applications for classifica\-tion and regression.
Additionally, decision trees are considered the basis for many other ML techniques, such as random forests, bagging, and boosted decision trees.
The idea of a decision tree is to represent the whole data as a tree where each internal node represents a test on an attribute (a decision rule), and each branch represents an outcome of the test; finally, each leaf node (terminal node) holds the label of the class.

Decision tree can be divided into two categories:
\begin{enumerate}
	\item Categorical variable decision trees: which includes categorical target variables that are divided into categories. A category means that the decision falls into one of the categories and there is no in-between such as (Yes/No category).
	\item Continuous variable decision trees: which has a continuous target variable that can be predicted based on available information (e.g. crack length).
\end{enumerate}
Figure~\ref{fig:Decision_tree} presents a typical decision tree.
Any decision tree has a root node where data input is carried through.
Furthermore, the root node is split into sets of decision rules that result either in a leaf node, which is a non-splitting node, or in another decision rule, creating what is so-called a branch or sub-tree.
In cases where there are decision rules that can be eliminated from the tree, a process called "pruning" is applied to minimize the complexity of the algorithm.
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[scale=1]{Figures/Chapter_3/decision_tree.png}
	\end{center}
	\caption{Decision tree.}
	\label{fig:Decision_tree}
\end{figure} 